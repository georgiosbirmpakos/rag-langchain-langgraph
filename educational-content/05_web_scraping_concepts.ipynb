{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🕷️ Web Scraping Concepts - Greek Derby RAG Chatbot\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this lesson, you will understand:\n",
        "- Web scraping fundamentals and best practices\n",
        "- HTML parsing and CSS selector strategies\n",
        "- Anti-bot detection and evasion techniques\n",
        "- Data cleaning and preprocessing methods\n",
        "- Rate limiting and ethical scraping\n",
        "- Error handling and resilience patterns\n",
        "- Integration with RAG systems\n",
        "- Legal and ethical considerations\n",
        "\n",
        "---\n",
        "\n",
        "## Q1: What is web scraping and how do we use it in our Greek Derby chatbot?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Web scraping is the automated extraction of data from websites. Our Greek Derby RAG chatbot uses web scraping to gather real-time information about Olympiakos and Panathinaikos from Gazzetta.gr, creating a dynamic knowledge base that stays current with the latest news and updates.\n",
        "\n",
        "### What is Web Scraping?\n",
        "\n",
        "**Web Scraping Definition:**\n",
        "Web scraping (also called web harvesting or web data extraction) is the process of automatically extracting data from websites using software tools. It involves:\n",
        "\n",
        "- **HTTP Requests**: Sending requests to web servers\n",
        "- **HTML Parsing**: Extracting structured data from HTML content\n",
        "- **Data Processing**: Cleaning and transforming raw data\n",
        "- **Storage**: Saving data for further use\n",
        "\n",
        "### Why Web Scraping for Our RAG Chatbot?\n",
        "\n",
        "**Traditional Knowledge Base Limitations:**\n",
        "- **Static Content**: Pre-written content becomes outdated\n",
        "- **Limited Scope**: Cannot cover all possible questions\n",
        "- **Manual Updates**: Requires constant manual maintenance\n",
        "- **No Real-time Data**: Cannot provide current information\n",
        "\n",
        "**Web Scraping Benefits:**\n",
        "- **Real-time Updates**: Always current information\n",
        "- **Comprehensive Coverage**: Access to vast amounts of data\n",
        "- **Automated Updates**: No manual intervention needed\n",
        "- **Dynamic Content**: Fresh news and statistics\n",
        "\n",
        "### Our Web Scraping Architecture:\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│                    Web Scraping Pipeline                    │\n",
        "│                                                             │\n",
        "│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐     │\n",
        "│  │   Gazzetta  │    │   HTML      │    │   Content   │     │\n",
        "│  │   .gr URLs  │───▶│   Parser    │───▶│   Processor │     │\n",
        "│  └─────────────┘    └─────────────┘    └─────────────┘     │\n",
        "│                                                             │\n",
        "│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐     │\n",
        "│  │   Text      │    │   Vector    │    │   RAG      │     │\n",
        "│  │   Splitter  │───▶│   Database  │───▶│   System   │     │\n",
        "│  └─────────────┘    └─────────────┘    └─────────────┘     │\n",
        "└─────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### Our Scraping Implementation:\n",
        "\n",
        "#### 1. **Target Websites**\n",
        "```python\n",
        "# backend/standalone-service/greek_derby_chatbot.py\n",
        "greek_derby_urls = [\n",
        "    \"https://www.gazzetta.gr/football/superleague/olympiakos\",\n",
        "    \"https://www.gazzetta.gr/football/superleague/panathinaikos\", \n",
        "    \"https://www.gazzetta.gr/football/superleague\",\n",
        "    \"https://www.gazzetta.gr\",\n",
        "]\n",
        "```\n",
        "\n",
        "**Why These URLs:**\n",
        "- **Official Source**: Gazzetta.gr is a major Greek sports news site\n",
        "- **Comprehensive Coverage**: Covers both teams and general football news\n",
        "- **Regular Updates**: Fresh content published daily\n",
        "- **Greek Language**: Native Greek content for better understanding\n",
        "\n",
        "#### 2. **Scraping Libraries Used**\n",
        "```python\n",
        "# Web scraping imports\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "import requests\n",
        "```\n",
        "\n",
        "**Library Choices:**\n",
        "- **LangChain WebBaseLoader**: High-level abstraction for web scraping\n",
        "- **BeautifulSoup (bs4)**: Powerful HTML parsing library\n",
        "- **Requests**: Low-level HTTP library for custom requests\n",
        "\n",
        "#### 3. **Multi-Strategy Scraping Approach**\n",
        "```python\n",
        "# backend/scheduler/update_vector_db.py\n",
        "def load_fresh_content(self) -> List[Document]:\n",
        "    \"\"\"Load fresh content from Gazzetta.gr\"\"\"\n",
        "    \n",
        "    for url in self.greek_derby_urls:\n",
        "        try:\n",
        "            # Strategy 1: CSS Selector-based scraping\n",
        "            loader = WebBaseLoader(\n",
        "                web_paths=(url,),\n",
        "                bs_kwargs=dict(\n",
        "                    parse_only=bs4.SoupStrainer(\n",
        "                        class_=(\"article-content\", \"article-title\", \"article-body\", \n",
        "                               \"content\", \"post-content\", \"entry-content\", \n",
        "                               \"post-body\", \"article-text\", \"main-content\", \n",
        "                               \"story-content\", \"article\", \"post\", \n",
        "                               \"content-area\", \"main\", \"body\")\n",
        "                    )\n",
        "                ),\n",
        "            )\n",
        "            docs = loader.load()\n",
        "            \n",
        "            # Strategy 2: Fallback without selectors\n",
        "            if not docs or all(len(doc.page_content.strip()) < 100 for doc in docs):\n",
        "                loader_fallback = WebBaseLoader(web_paths=(url,))\n",
        "                docs = loader_fallback.load()\n",
        "            \n",
        "            # Strategy 3: Content validation and filtering\n",
        "            valid_docs = [doc for doc in docs if len(doc.page_content.strip()) > 50]\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load {url}: {str(e)}\")\n",
        "            continue\n",
        "```\n",
        "\n",
        "### Scraping Strategy Breakdown:\n",
        "\n",
        "#### 1. **Primary Strategy: CSS Selector Targeting**\n",
        "```python\n",
        "# Target specific HTML elements\n",
        "bs4.SoupStrainer(\n",
        "    class_=(\"article-content\", \"article-title\", \"article-body\", \n",
        "           \"content\", \"post-content\", \"entry-content\", \n",
        "           \"post-body\", \"article-text\", \"main-content\", \n",
        "           \"story-content\", \"article\", \"post\", \n",
        "           \"content-area\", \"main\", \"body\")\n",
        ")\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- **Precision**: Extract only relevant content\n",
        "- **Efficiency**: Skip navigation, ads, and irrelevant elements\n",
        "- **Clean Data**: Better quality extracted content\n",
        "- **Performance**: Faster processing with less data\n",
        "\n",
        "#### 2. **Fallback Strategy: Full Page Scraping**\n",
        "```python\n",
        "# If selectors fail, scrape entire page\n",
        "loader_fallback = WebBaseLoader(web_paths=(url,))\n",
        "docs = loader_fallback.load()\n",
        "```\n",
        "\n",
        "**When to Use:**\n",
        "- **Selector Failure**: When CSS selectors don't match\n",
        "- **Site Changes**: When website structure changes\n",
        "- **New Content Types**: For different article layouts\n",
        "- **Backup Method**: Ensures we don't miss content\n",
        "\n",
        "#### 3. **Content Validation and Filtering**\n",
        "```python\n",
        "# Filter out low-quality content\n",
        "valid_docs = [doc for doc in docs if len(doc.page_content.strip()) > 50]\n",
        "\n",
        "# Additional filtering\n",
        "if valid_docs:\n",
        "    for doc in valid_docs:\n",
        "        # Add metadata\n",
        "        doc.metadata.update({\n",
        "            'source': url,\n",
        "            'scraped_at': datetime.now().isoformat(),\n",
        "            'content_type': 'greek_football_news'\n",
        "        })\n",
        "```\n",
        "\n",
        "**Quality Filters:**\n",
        "- **Minimum Length**: Remove very short content (< 50 characters)\n",
        "- **Content Type**: Focus on article content, not navigation\n",
        "- **Language**: Ensure Greek language content\n",
        "- **Relevance**: Filter for football-related content\n",
        "\n",
        "### Anti-Bot Detection and Evasion:\n",
        "\n",
        "#### 1. **User Agent Rotation**\n",
        "```python\n",
        "# Set realistic user agent\n",
        "os.environ['USER_AGENT'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "```\n",
        "\n",
        "**Why User Agent Matters:**\n",
        "- **Bot Detection**: Websites block requests with default user agents\n",
        "- **Realistic Simulation**: Mimic real browser behavior\n",
        "- **Rate Limiting**: Some sites have different limits for different user agents\n",
        "\n",
        "#### 2. **Request Headers**\n",
        "```python\n",
        "# Add realistic headers\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.5',\n",
        "    'Accept-Encoding': 'gzip, deflate',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Upgrade-Insecure-Requests': '1',\n",
        "}\n",
        "```\n",
        "\n",
        "#### 3. **Rate Limiting and Delays**\n",
        "```python\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Add delays between requests\n",
        "time.sleep(random.uniform(1, 3))  # Random delay between 1-3 seconds\n",
        "\n",
        "# Respect robots.txt\n",
        "# Check robots.txt before scraping\n",
        "```\n",
        "\n",
        "### Error Handling and Resilience:\n",
        "\n",
        "#### 1. **Try-Catch Blocks**\n",
        "```python\n",
        "for url in self.greek_derby_urls:\n",
        "    try:\n",
        "        # Scraping logic\n",
        "        docs = loader.load()\n",
        "        # Process documents\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        self.logger.error(f\"Network error for {url}: {str(e)}\")\n",
        "        continue\n",
        "    except bs4.exceptions.ParserError as e:\n",
        "        self.logger.error(f\"Parsing error for {url}: {str(e)}\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        self.logger.error(f\"Unexpected error for {url}: {str(e)}\")\n",
        "        continue\n",
        "```\n",
        "\n",
        "#### 2. **Retry Logic**\n",
        "```python\n",
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def retry_on_failure(max_retries=3, delay=1):\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                except Exception as e:\n",
        "                    if attempt == max_retries - 1:\n",
        "                        raise e\n",
        "                    time.sleep(delay * (2 ** attempt))  # Exponential backoff\n",
        "            return None\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "@retry_on_failure(max_retries=3, delay=2)\n",
        "def scrape_url(url):\n",
        "    # Scraping logic here\n",
        "    pass\n",
        "```\n",
        "\n",
        "### Integration with RAG System:\n",
        "\n",
        "#### 1. **Document Processing**\n",
        "```python\n",
        "# Process scraped content for RAG\n",
        "def process_scraped_content(self, docs):\n",
        "    \"\"\"Process scraped documents for RAG system\"\"\"\n",
        "    \n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    # Split documents\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "    \n",
        "    # Add metadata\n",
        "    for doc in split_docs:\n",
        "        doc.metadata.update({\n",
        "            'source_type': 'web_scraped',\n",
        "            'language': 'greek',\n",
        "            'domain': 'gazzetta.gr'\n",
        "        })\n",
        "    \n",
        "    return split_docs\n",
        "```\n",
        "\n",
        "#### 2. **Vector Database Integration**\n",
        "```python\n",
        "# Store processed content in vector database\n",
        "def store_in_vector_db(self, processed_docs):\n",
        "    \"\"\"Store processed documents in Pinecone\"\"\"\n",
        "    \n",
        "    # Add to vector store\n",
        "    self.vector_store.add_documents(processed_docs)\n",
        "    \n",
        "    # Log success\n",
        "    self.logger.info(f\"Stored {len(processed_docs)} documents in vector database\")\n",
        "```\n",
        "\n",
        "### Benefits for Our RAG Chatbot:\n",
        "\n",
        "1. **Real-time Information**: Always current news and updates\n",
        "2. **Comprehensive Knowledge**: Access to vast amounts of Greek football content\n",
        "3. **Automated Updates**: No manual maintenance required\n",
        "4. **Dynamic Responses**: Can answer questions about recent events\n",
        "5. **Language Accuracy**: Native Greek content for better understanding\n",
        "6. **Scalable Data**: Can easily add more sources\n",
        "7. **Cost Effective**: Free content from public websites\n",
        "8. **Reliable Source**: Gazzetta.gr is a trusted Greek sports news source\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2: How do we implement HTML parsing and CSS selector strategies?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "HTML parsing and CSS selector strategies are crucial for extracting relevant content from web pages. Our Greek Derby chatbot uses sophisticated parsing techniques to accurately extract Greek football content while filtering out irrelevant elements like navigation, ads, and scripts.\n",
        "\n",
        "### HTML Parsing Fundamentals:\n",
        "\n",
        "#### 1. **BeautifulSoup Integration with LangChain**\n",
        "\n",
        "```python\n",
        "# backend/scheduler/update_vector_db.py\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "\n",
        "# Configure BeautifulSoup parsing\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(url,),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"article-content\", \"article-title\", \"article-body\", \n",
        "                   \"content\", \"post-content\", \"entry-content\", \n",
        "                   \"post-body\", \"article-text\", \"main-content\", \n",
        "                   \"story-content\", \"article\", \"post\", \n",
        "                   \"content-area\", \"main\", \"body\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "```\n",
        "\n",
        "**Why BeautifulSoup:**\n",
        "- **Flexible Parsing**: Handles malformed HTML gracefully\n",
        "- **CSS Selectors**: Powerful element selection capabilities\n",
        "- **Integration**: Works seamlessly with LangChain\n",
        "- **Performance**: Efficient parsing of large documents\n",
        "\n",
        "#### 2. **CSS Selector Strategy**\n",
        "\n",
        "```python\n",
        "# Multi-level CSS selector approach\n",
        "css_selectors = {\n",
        "    'primary': [\n",
        "        'article-content',      # Main article content\n",
        "        'article-title',        # Article headlines\n",
        "        'article-body',         # Article body text\n",
        "        'content',              # Generic content area\n",
        "        'post-content',         # Blog post content\n",
        "        'entry-content',        # WordPress entry content\n",
        "        'post-body',            # Post body text\n",
        "        'article-text',         # Article text content\n",
        "        'main-content',         # Main content area\n",
        "        'story-content',        # News story content\n",
        "    ],\n",
        "    'secondary': [\n",
        "        'article',              # Article elements\n",
        "        'post',                 # Post elements\n",
        "        'content-area',         # Content area\n",
        "        'main',                 # Main section\n",
        "        'body',                 # Body content\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Selector Hierarchy:**\n",
        "1. **Primary Selectors**: Target specific content containers\n",
        "2. **Secondary Selectors**: Fallback to broader content areas\n",
        "3. **Tertiary Selectors**: Last resort for general content\n",
        "\n",
        "### Advanced Parsing Techniques:\n",
        "\n",
        "#### 1. **Content Type Detection**\n",
        "\n",
        "```python\n",
        "def detect_content_type(soup):\n",
        "    \"\"\"Detect the type of content being parsed\"\"\"\n",
        "    \n",
        "    # Check for article indicators\n",
        "    if soup.find('article'):\n",
        "        return 'article'\n",
        "    \n",
        "    # Check for news indicators\n",
        "    if soup.find(class_=['news', 'story', 'post']):\n",
        "        return 'news'\n",
        "    \n",
        "    # Check for list indicators\n",
        "    if soup.find(['ul', 'ol']):\n",
        "        return 'list'\n",
        "    \n",
        "    # Default to general content\n",
        "    return 'general'\n",
        "\n",
        "# Usage in parsing\n",
        "content_type = detect_content_type(soup)\n",
        "if content_type == 'article':\n",
        "    # Use article-specific selectors\n",
        "    selectors = ['article-content', 'article-body', 'entry-content']\n",
        "else:\n",
        "    # Use general selectors\n",
        "    selectors = ['content', 'main', 'body']\n",
        "```\n",
        "\n",
        "#### 2. **Dynamic Selector Adaptation**\n",
        "\n",
        "```python\n",
        "def find_best_selectors(soup, url):\n",
        "    \"\"\"Dynamically find the best CSS selectors for a page\"\"\"\n",
        "    \n",
        "    # Common content patterns\n",
        "    content_patterns = [\n",
        "        {'tag': 'div', 'class': 'article-content'},\n",
        "        {'tag': 'article', 'class': 'post'},\n",
        "        {'tag': 'div', 'class': 'content'},\n",
        "        {'tag': 'main', 'class': 'main-content'},\n",
        "        {'tag': 'section', 'class': 'story'},\n",
        "    ]\n",
        "    \n",
        "    best_selectors = []\n",
        "    \n",
        "    for pattern in content_patterns:\n",
        "        elements = soup.find_all(pattern['tag'], class_=pattern['class'])\n",
        "        if elements:\n",
        "            # Check content quality\n",
        "            total_text = sum(len(elem.get_text().strip()) for elem in elements)\n",
        "            if total_text > 100:  # Minimum content threshold\n",
        "                best_selectors.append(pattern)\n",
        "    \n",
        "    return best_selectors\n",
        "```\n",
        "\n",
        "#### 3. **Content Quality Assessment**\n",
        "\n",
        "```python\n",
        "def assess_content_quality(element):\n",
        "    \"\"\"Assess the quality of extracted content\"\"\"\n",
        "    \n",
        "    text = element.get_text().strip()\n",
        "    \n",
        "    # Quality metrics\n",
        "    metrics = {\n",
        "        'length': len(text),\n",
        "        'word_count': len(text.split()),\n",
        "        'paragraph_count': len(element.find_all('p')),\n",
        "        'link_density': len(element.find_all('a')) / max(len(text.split()), 1),\n",
        "        'has_headings': bool(element.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])),\n",
        "        'has_images': bool(element.find_all('img')),\n",
        "    }\n",
        "    \n",
        "    # Calculate quality score\n",
        "    quality_score = 0\n",
        "    \n",
        "    # Length bonus\n",
        "    if metrics['length'] > 200:\n",
        "        quality_score += 2\n",
        "    elif metrics['length'] > 100:\n",
        "        quality_score += 1\n",
        "    \n",
        "    # Word count bonus\n",
        "    if metrics['word_count'] > 50:\n",
        "        quality_score += 2\n",
        "    elif metrics['word_count'] > 20:\n",
        "        quality_score += 1\n",
        "    \n",
        "    # Structure bonus\n",
        "    if metrics['has_headings']:\n",
        "        quality_score += 1\n",
        "    \n",
        "    # Link density penalty\n",
        "    if metrics['link_density'] > 0.3:\n",
        "        quality_score -= 1\n",
        "    \n",
        "    return quality_score, metrics\n",
        "```\n",
        "\n",
        "### Greek Language Specific Parsing:\n",
        "\n",
        "#### 1. **Greek Text Detection**\n",
        "\n",
        "```python\n",
        "import re\n",
        "\n",
        "def is_greek_text(text):\n",
        "    \"\"\"Check if text contains Greek characters\"\"\"\n",
        "    greek_pattern = re.compile(r'[\\u0370-\\u03FF\\u1F00-\\u1FFF]')\n",
        "    return bool(greek_pattern.search(text))\n",
        "\n",
        "def filter_greek_content(docs):\n",
        "    \"\"\"Filter documents to keep only Greek content\"\"\"\n",
        "    greek_docs = []\n",
        "    \n",
        "    for doc in docs:\n",
        "        if is_greek_text(doc.page_content):\n",
        "            greek_docs.append(doc)\n",
        "        else:\n",
        "            # Check if it's mixed content\n",
        "            greek_ratio = len(re.findall(r'[\\u0370-\\u03FF\\u1F00-\\u1FFF]', doc.page_content))\n",
        "            total_chars = len(doc.page_content)\n",
        "            \n",
        "            if greek_ratio / max(total_chars, 1) > 0.3:  # 30% Greek content\n",
        "                greek_docs.append(doc)\n",
        "    \n",
        "    return greek_docs\n",
        "```\n",
        "\n",
        "#### 2. **Greek-Specific Content Selectors**\n",
        "\n",
        "```python\n",
        "# Greek news site specific selectors\n",
        "greek_content_selectors = [\n",
        "    'article-content',\n",
        "    'article-title', \n",
        "    'article-body',\n",
        "    'news-content',\n",
        "    'story-content',\n",
        "    'post-content',\n",
        "    'entry-content',\n",
        "    'content',\n",
        "    'main-content',\n",
        "    'article',\n",
        "    'post',\n",
        "    'news-item',\n",
        "    'story',\n",
        "]\n",
        "\n",
        "# Greek sports specific selectors\n",
        "greek_sports_selectors = [\n",
        "    'match-report',\n",
        "    'team-news',\n",
        "    'player-news',\n",
        "    'transfer-news',\n",
        "    'match-preview',\n",
        "    'post-match',\n",
        "    'analysis',\n",
        "    'commentary',\n",
        "]\n",
        "```\n",
        "\n",
        "### Error Handling in Parsing:\n",
        "\n",
        "#### 1. **Parser Error Recovery**\n",
        "\n",
        "```python\n",
        "def robust_html_parsing(html_content, url):\n",
        "    \"\"\"Robust HTML parsing with error recovery\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Try with BeautifulSoup\n",
        "        soup = bs4.BeautifulSoup(html_content, 'html.parser')\n",
        "        return soup\n",
        "    except bs4.exceptions.ParserError as e:\n",
        "        print(f\"Parser error for {url}: {e}\")\n",
        "        \n",
        "        try:\n",
        "            # Try with lxml parser\n",
        "            soup = bs4.BeautifulSoup(html_content, 'lxml')\n",
        "            return soup\n",
        "        except Exception as e2:\n",
        "            print(f\"LXML parser error for {url}: {e2}\")\n",
        "            \n",
        "            try:\n",
        "                # Try with html5lib parser\n",
        "                soup = bs4.BeautifulSoup(html_content, 'html5lib')\n",
        "                return soup\n",
        "            except Exception as e3:\n",
        "                print(f\"HTML5lib parser error for {url}: {e3}\")\n",
        "                return None\n",
        "```\n",
        "\n",
        "#### 2. **Content Extraction Fallbacks**\n",
        "\n",
        "```python\n",
        "def extract_content_with_fallbacks(soup, url):\n",
        "    \"\"\"Extract content with multiple fallback strategies\"\"\"\n",
        "    \n",
        "    # Strategy 1: CSS selectors\n",
        "    content = extract_with_selectors(soup)\n",
        "    if content and len(content.strip()) > 100:\n",
        "        return content\n",
        "    \n",
        "    # Strategy 2: Tag-based extraction\n",
        "    content = extract_with_tags(soup)\n",
        "    if content and len(content.strip()) > 100:\n",
        "        return content\n",
        "    \n",
        "    # Strategy 3: Text-based extraction\n",
        "    content = extract_text_only(soup)\n",
        "    if content and len(content.strip()) > 100:\n",
        "        return content\n",
        "    \n",
        "    # Strategy 4: Full page text\n",
        "    content = soup.get_text()\n",
        "    return content\n",
        "\n",
        "def extract_with_selectors(soup):\n",
        "    \"\"\"Extract content using CSS selectors\"\"\"\n",
        "    selectors = ['article-content', 'content', 'main', 'article']\n",
        "    \n",
        "    for selector in selectors:\n",
        "        elements = soup.find_all(class_=selector)\n",
        "        if elements:\n",
        "            text = ' '.join(elem.get_text() for elem in elements)\n",
        "            if len(text.strip()) > 100:\n",
        "                return text\n",
        "    \n",
        "    return None\n",
        "\n",
        "def extract_with_tags(soup):\n",
        "    \"\"\"Extract content using HTML tags\"\"\"\n",
        "    # Remove script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.decompose()\n",
        "    \n",
        "    # Get text from main content tags\n",
        "    content_tags = ['article', 'main', 'section', 'div']\n",
        "    text_parts = []\n",
        "    \n",
        "    for tag in content_tags:\n",
        "        elements = soup.find_all(tag)\n",
        "        for element in elements:\n",
        "            text = element.get_text().strip()\n",
        "            if len(text) > 50:  # Minimum length\n",
        "                text_parts.append(text)\n",
        "    \n",
        "    return ' '.join(text_parts)\n",
        "```\n",
        "\n",
        "### Performance Optimization:\n",
        "\n",
        "#### 1. **Selective Parsing**\n",
        "\n",
        "```python\n",
        "def selective_parsing(html_content, target_selectors):\n",
        "    \"\"\"Parse only specific parts of HTML for better performance\"\"\"\n",
        "    \n",
        "    # Create a strainer for specific elements\n",
        "    strainer = bs4.SoupStrainer(\n",
        "        class_=target_selectors,\n",
        "        attrs={'id': re.compile(r'content|article|main|post')}\n",
        "    )\n",
        "    \n",
        "    # Parse only the strained content\n",
        "    soup = bs4.BeautifulSoup(html_content, 'html.parser', parse_only=strainer)\n",
        "    \n",
        "    return soup\n",
        "```\n",
        "\n",
        "#### 2. **Caching Parsed Content**\n",
        "\n",
        "```python\n",
        "import hashlib\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=100)\n",
        "def cached_parse(url, content_hash):\n",
        "    \"\"\"Cache parsed content to avoid re-parsing\"\"\"\n",
        "    # This would be implemented with actual caching logic\n",
        "    pass\n",
        "\n",
        "def parse_with_caching(url, html_content):\n",
        "    \"\"\"Parse HTML with caching\"\"\"\n",
        "    content_hash = hashlib.md5(html_content.encode()).hexdigest()\n",
        "    \n",
        "    # Check cache first\n",
        "    cached_result = cached_parse(url, content_hash)\n",
        "    if cached_result:\n",
        "        return cached_result\n",
        "    \n",
        "    # Parse and cache\n",
        "    result = robust_html_parsing(html_content, url)\n",
        "    return result\n",
        "```\n",
        "\n",
        "### Best Practices for CSS Selectors:\n",
        "\n",
        "#### 1. **Selector Specificity**\n",
        "\n",
        "```python\n",
        "# Good: Specific selectors\n",
        "selectors = [\n",
        "    'article.news-item .content',\n",
        "    'div.article-body p',\n",
        "    'section.main-content article',\n",
        "]\n",
        "\n",
        "# Bad: Too generic\n",
        "selectors = [\n",
        "    'div',\n",
        "    'p',\n",
        "    'span',\n",
        "]\n",
        "```\n",
        "\n",
        "#### 2. **Fallback Hierarchy**\n",
        "\n",
        "```python\n",
        "def get_content_with_hierarchy(soup):\n",
        "    \"\"\"Get content using a hierarchy of selectors\"\"\"\n",
        "    \n",
        "    hierarchy = [\n",
        "        # Most specific\n",
        "        'article.news-item .content',\n",
        "        'div.article-body',\n",
        "        'section.main-content',\n",
        "        \n",
        "        # Less specific\n",
        "        'article',\n",
        "        'div.content',\n",
        "        'main',\n",
        "        \n",
        "        # Fallback\n",
        "        'body'\n",
        "    ]\n",
        "    \n",
        "    for selector in hierarchy:\n",
        "        elements = soup.select(selector)\n",
        "        if elements:\n",
        "            content = ' '.join(elem.get_text() for elem in elements)\n",
        "            if len(content.strip()) > 100:\n",
        "                return content\n",
        "    \n",
        "    return None\n",
        "```\n",
        "\n",
        "### Benefits for Our RAG Chatbot:\n",
        "\n",
        "1. **Accurate Content Extraction**: Precise targeting of relevant content\n",
        "2. **Noise Reduction**: Filter out ads, navigation, and irrelevant elements\n",
        "3. **Language Focus**: Prioritize Greek language content\n",
        "4. **Quality Assurance**: Ensure high-quality content for the knowledge base\n",
        "5. **Performance**: Efficient parsing with minimal resource usage\n",
        "6. **Reliability**: Robust error handling and fallback strategies\n",
        "7. **Maintainability**: Easy to update selectors when sites change\n",
        "8. **Scalability**: Can handle multiple content types and sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q3: How do we handle anti-bot detection and implement ethical scraping?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Anti-bot detection and ethical scraping are crucial aspects of web scraping. Our Greek Derby chatbot implements sophisticated techniques to avoid detection while respecting website resources and terms of service, ensuring sustainable and responsible data collection.\n",
        "\n",
        "### Understanding Anti-Bot Detection:\n",
        "\n",
        "#### 1. **Common Bot Detection Methods**\n",
        "\n",
        "```python\n",
        "# Websites use various methods to detect bots\n",
        "detection_methods = {\n",
        "    'user_agent_check': 'Analyze User-Agent headers',\n",
        "    'request_frequency': 'Monitor request rate and patterns',\n",
        "    'javascript_challenges': 'Require JavaScript execution',\n",
        "    'captcha_systems': 'Human verification challenges',\n",
        "    'ip_blacklisting': 'Block suspicious IP addresses',\n",
        "    'behavioral_analysis': 'Analyze browsing patterns',\n",
        "    'header_validation': 'Check for missing or suspicious headers',\n",
        "    'cookie_handling': 'Require proper cookie management',\n",
        "    'session_management': 'Track session continuity',\n",
        "    'device_fingerprinting': 'Identify unique device characteristics'\n",
        "}\n",
        "```\n",
        "\n",
        "#### 2. **Our Anti-Detection Strategy**\n",
        "\n",
        "```python\n",
        "# backend/standalone-service/greek_derby_chatbot.py\n",
        "def setup_anti_detection():\n",
        "    \"\"\"Configure anti-detection measures\"\"\"\n",
        "    \n",
        "    # Set realistic user agent\n",
        "    os.environ['USER_AGENT'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    \n",
        "    # Configure headers\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Accept-Encoding': 'gzip, deflate',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "        'Cache-Control': 'max-age=0',\n",
        "        'DNT': '1',\n",
        "        'Sec-Fetch-Dest': 'document',\n",
        "        'Sec-Fetch-Mode': 'navigate',\n",
        "        'Sec-Fetch-Site': 'none',\n",
        "        'Sec-Fetch-User': '?1',\n",
        "    }\n",
        "    \n",
        "    return headers\n",
        "```\n",
        "\n",
        "### Advanced Anti-Detection Techniques:\n",
        "\n",
        "#### 1. **User Agent Rotation**\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "class UserAgentRotator:\n",
        "    \"\"\"Rotate user agents to avoid detection\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.user_agents = [\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
        "        ]\n",
        "    \n",
        "    def get_random_user_agent(self):\n",
        "        \"\"\"Get a random user agent\"\"\"\n",
        "        return random.choice(self.user_agents)\n",
        "    \n",
        "    def get_headers(self):\n",
        "        \"\"\"Get headers with random user agent\"\"\"\n",
        "        return {\n",
        "            'User-Agent': self.get_random_user_agent(),\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        }\n",
        "```\n",
        "\n",
        "#### 2. **Request Timing and Rate Limiting**\n",
        "\n",
        "```python\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "class RateLimiter:\n",
        "    \"\"\"Implement intelligent rate limiting\"\"\"\n",
        "    \n",
        "    def __init__(self, min_delay=1, max_delay=3, burst_limit=5):\n",
        "        self.min_delay = min_delay\n",
        "        self.max_delay = max_delay\n",
        "        self.burst_limit = burst_limit\n",
        "        self.request_times = []\n",
        "        self.last_request_time = 0\n",
        "    \n",
        "    def wait_if_needed(self):\n",
        "        \"\"\"Wait if rate limit would be exceeded\"\"\"\n",
        "        now = time.time()\n",
        "        \n",
        "        # Remove old request times (older than 1 minute)\n",
        "        self.request_times = [t for t in self.request_times if now - t < 60]\n",
        "        \n",
        "        # Check burst limit\n",
        "        if len(self.request_times) >= self.burst_limit:\n",
        "            sleep_time = 60 - (now - self.request_times[0])\n",
        "            if sleep_time > 0:\n",
        "                time.sleep(sleep_time)\n",
        "                self.request_times = []\n",
        "        \n",
        "        # Random delay between requests\n",
        "        delay = random.uniform(self.min_delay, self.max_delay)\n",
        "        time.sleep(delay)\n",
        "        \n",
        "        # Record this request\n",
        "        self.request_times.append(now)\n",
        "        self.last_request_time = now\n",
        "\n",
        "# Usage in scraping\n",
        "rate_limiter = RateLimiter(min_delay=2, max_delay=5, burst_limit=3)\n",
        "\n",
        "for url in urls:\n",
        "    rate_limiter.wait_if_needed()\n",
        "    # Make request\n",
        "    response = requests.get(url, headers=headers)\n",
        "```\n",
        "\n",
        "#### 3. **Session Management**\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "class SessionManager:\n",
        "    \"\"\"Manage HTTP sessions with proper configuration\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.setup_session()\n",
        "    \n",
        "    def setup_session(self):\n",
        "        \"\"\"Configure session with retry strategy\"\"\"\n",
        "        \n",
        "        # Retry strategy\n",
        "        retry_strategy = Retry(\n",
        "            total=3,\n",
        "            backoff_factor=1,\n",
        "            status_forcelist=[429, 500, 502, 503, 504],\n",
        "        )\n",
        "        \n",
        "        # Mount adapter\n",
        "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "        self.session.mount(\"http://\", adapter)\n",
        "        self.session.mount(\"https://\", adapter)\n",
        "        \n",
        "        # Set default headers\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "        })\n",
        "    \n",
        "    def get(self, url, **kwargs):\n",
        "        \"\"\"Make GET request with session\"\"\"\n",
        "        return self.session.get(url, **kwargs)\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Close session\"\"\"\n",
        "        self.session.close()\n",
        "```\n",
        "\n",
        "### Ethical Scraping Practices:\n",
        "\n",
        "#### 1. **Robots.txt Compliance**\n",
        "\n",
        "```python\n",
        "import urllib.robotparser\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "class RobotsTxtChecker:\n",
        "    \"\"\"Check robots.txt before scraping\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.robots_cache = {}\n",
        "    \n",
        "    def can_scrape(self, url, user_agent='*'):\n",
        "        \"\"\"Check if URL can be scraped according to robots.txt\"\"\"\n",
        "        \n",
        "        parsed_url = urlparse(url)\n",
        "        robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
        "        \n",
        "        # Check cache first\n",
        "        if robots_url in self.robots_cache:\n",
        "            rp = self.robots_cache[robots_url]\n",
        "        else:\n",
        "            rp = urllib.robotparser.RobotFileParser()\n",
        "            rp.set_url(robots_url)\n",
        "            rp.read()\n",
        "            self.robots_cache[robots_url] = rp\n",
        "        \n",
        "        return rp.can_fetch(user_agent, url)\n",
        "    \n",
        "    def get_crawl_delay(self, url, user_agent='*'):\n",
        "        \"\"\"Get recommended crawl delay from robots.txt\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
        "        \n",
        "        if robots_url in self.robots_cache:\n",
        "            rp = self.robots_cache[robots_url]\n",
        "            return rp.crawl_delay(user_agent)\n",
        "        \n",
        "        return None\n",
        "\n",
        "# Usage\n",
        "robots_checker = RobotsTxtChecker()\n",
        "\n",
        "for url in urls:\n",
        "    if robots_checker.can_scrape(url):\n",
        "        delay = robots_checker.get_crawl_delay(url)\n",
        "        if delay:\n",
        "            time.sleep(delay)\n",
        "        # Proceed with scraping\n",
        "    else:\n",
        "        print(f\"Scraping not allowed for {url}\")\n",
        "```\n",
        "\n",
        "#### 2. **Respectful Request Patterns**\n",
        "\n",
        "```python\n",
        "class EthicalScraper:\n",
        "    \"\"\"Implement ethical scraping practices\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.robots_checker = RobotsTxtChecker()\n",
        "        self.rate_limiter = RateLimiter()\n",
        "        self.session_manager = SessionManager()\n",
        "    \n",
        "    def scrape_respectfully(self, urls):\n",
        "        \"\"\"Scrape URLs with ethical considerations\"\"\"\n",
        "        \n",
        "        for url in urls:\n",
        "            try:\n",
        "                # Check robots.txt\n",
        "                if not self.robots_checker.can_scrape(url):\n",
        "                    print(f\"Skipping {url} - not allowed by robots.txt\")\n",
        "                    continue\n",
        "                \n",
        "                # Apply rate limiting\n",
        "                self.rate_limiter.wait_if_needed()\n",
        "                \n",
        "                # Make request\n",
        "                response = self.session_manager.get(url, timeout=30)\n",
        "                \n",
        "                # Check response status\n",
        "                if response.status_code == 200:\n",
        "                    yield response\n",
        "                elif response.status_code == 429:  # Too Many Requests\n",
        "                    print(f\"Rate limited for {url}, waiting longer...\")\n",
        "                    time.sleep(60)  # Wait 1 minute\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"Error {response.status_code} for {url}\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping {url}: {e}\")\n",
        "                continue\n",
        "```\n",
        "\n",
        "#### 3. **Data Usage Guidelines**\n",
        "\n",
        "```python\n",
        "class DataUsagePolicy:\n",
        "    \"\"\"Define data usage policies and restrictions\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.policies = {\n",
        "            'max_requests_per_hour': 100,\n",
        "            'max_requests_per_day': 1000,\n",
        "            'respect_noindex': True,\n",
        "            'respect_nofollow': True,\n",
        "            'cache_duration': 3600,  # 1 hour\n",
        "            'user_agent_identification': True,\n",
        "            'contact_info_required': True,\n",
        "        }\n",
        "    \n",
        "    def validate_request(self, url, request_count):\n",
        "        \"\"\"Validate if request is within policy limits\"\"\"\n",
        "        \n",
        "        # Check hourly limit\n",
        "        if request_count['hourly'] >= self.policies['max_requests_per_hour']:\n",
        "            return False, \"Hourly limit exceeded\"\n",
        "        \n",
        "        # Check daily limit\n",
        "        if request_count['daily'] >= self.policies['max_requests_per_day']:\n",
        "            return False, \"Daily limit exceeded\"\n",
        "        \n",
        "        return True, \"Request allowed\"\n",
        "    \n",
        "    def get_cache_duration(self):\n",
        "        \"\"\"Get recommended cache duration\"\"\"\n",
        "        return self.policies['cache_duration']\n",
        "```\n",
        "\n",
        "### Advanced Anti-Detection Techniques:\n",
        "\n",
        "#### 1. **Proxy Rotation**\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "class ProxyRotator:\n",
        "    \"\"\"Rotate proxies to avoid IP blocking\"\"\"\n",
        "    \n",
        "    def __init__(self, proxy_list):\n",
        "        self.proxy_list = proxy_list\n",
        "        self.current_proxy = None\n",
        "    \n",
        "    def get_random_proxy(self):\n",
        "        \"\"\"Get a random proxy from the list\"\"\"\n",
        "        return random.choice(self.proxy_list)\n",
        "    \n",
        "    def get_proxy_dict(self):\n",
        "        \"\"\"Get proxy configuration for requests\"\"\"\n",
        "        proxy = self.get_random_proxy()\n",
        "        return {\n",
        "            'http': proxy,\n",
        "            'https': proxy\n",
        "        }\n",
        "    \n",
        "    def rotate_proxy(self):\n",
        "        \"\"\"Rotate to a new proxy\"\"\"\n",
        "        self.current_proxy = self.get_random_proxy()\n",
        "\n",
        "# Usage with proxies\n",
        "proxy_rotator = ProxyRotator([\n",
        "    'http://proxy1:port',\n",
        "    'http://proxy2:port',\n",
        "    'http://proxy3:port',\n",
        "])\n",
        "\n",
        "# Make request with proxy\n",
        "response = requests.get(url, proxies=proxy_rotator.get_proxy_dict())\n",
        "```\n",
        "\n",
        "#### 2. **JavaScript Rendering (Selenium)**\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "class JavaScriptScraper:\n",
        "    \"\"\"Handle JavaScript-heavy websites\"\"\"\n",
        "    \n",
        "    def __init__(self, headless=True):\n",
        "        self.driver = self.setup_driver(headless)\n",
        "    \n",
        "    def setup_driver(self, headless):\n",
        "        \"\"\"Setup Chrome driver with options\"\"\"\n",
        "        options = Options()\n",
        "        \n",
        "        if headless:\n",
        "            options.add_argument('--headless')\n",
        "        \n",
        "        # Anti-detection options\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument('--disable-dev-shm-usage')\n",
        "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        options.add_experimental_option('useAutomationExtension', False)\n",
        "        \n",
        "        driver = webdriver.Chrome(options=options)\n",
        "        \n",
        "        # Execute script to remove webdriver property\n",
        "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        \n",
        "        return driver\n",
        "    \n",
        "    def scrape_with_js(self, url):\n",
        "        \"\"\"Scrape URL that requires JavaScript\"\"\"\n",
        "        try:\n",
        "            self.driver.get(url)\n",
        "            \n",
        "            # Wait for content to load\n",
        "            WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "            )\n",
        "            \n",
        "            # Get page source\n",
        "            page_source = self.driver.page_source\n",
        "            \n",
        "            return page_source\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Close the driver\"\"\"\n",
        "        self.driver.quit()\n",
        "```\n",
        "\n",
        "#### 3. **Request Fingerprinting Avoidance**\n",
        "\n",
        "```python\n",
        "class FingerprintAvoidance:\n",
        "    \"\"\"Avoid request fingerprinting\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def randomize_headers():\n",
        "        \"\"\"Randomize headers to avoid fingerprinting\"\"\"\n",
        "        base_headers = {\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        }\n",
        "        \n",
        "        # Randomly add optional headers\n",
        "        optional_headers = {\n",
        "            'DNT': '1',\n",
        "            'Cache-Control': 'max-age=0',\n",
        "            'Sec-Fetch-Dest': 'document',\n",
        "            'Sec-Fetch-Mode': 'navigate',\n",
        "            'Sec-Fetch-Site': 'none',\n",
        "            'Sec-Fetch-User': '?1',\n",
        "        }\n",
        "        \n",
        "        # Randomly include some optional headers\n",
        "        for header, value in optional_headers.items():\n",
        "            if random.random() > 0.5:\n",
        "                base_headers[header] = value\n",
        "        \n",
        "        return base_headers\n",
        "    \n",
        "    @staticmethod\n",
        "    def randomize_request_timing():\n",
        "        \"\"\"Randomize request timing patterns\"\"\"\n",
        "        # Random delay between 1-5 seconds\n",
        "        delay = random.uniform(1, 5)\n",
        "        \n",
        "        # Sometimes add longer delays\n",
        "        if random.random() < 0.1:  # 10% chance\n",
        "            delay += random.uniform(10, 30)\n",
        "        \n",
        "        return delay\n",
        "```\n",
        "\n",
        "### Legal and Ethical Considerations:\n",
        "\n",
        "#### 1. **Terms of Service Compliance**\n",
        "\n",
        "```python\n",
        "class TermsOfServiceChecker:\n",
        "    \"\"\"Check and comply with terms of service\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.known_restrictions = {\n",
        "            'gazzetta.gr': {\n",
        "                'allowed': True,\n",
        "                'rate_limit': 60,  # requests per hour\n",
        "                'requires_attribution': True,\n",
        "                'commercial_use': False,\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def check_tos(self, domain):\n",
        "        \"\"\"Check terms of service for domain\"\"\"\n",
        "        return self.known_restrictions.get(domain, {\n",
        "            'allowed': True,\n",
        "            'rate_limit': 30,\n",
        "            'requires_attribution': False,\n",
        "            'commercial_use': False,\n",
        "        })\n",
        "    \n",
        "    def is_compliant(self, domain, request_rate):\n",
        "        \"\"\"Check if current usage is compliant\"\"\"\n",
        "        tos = self.check_tos(domain)\n",
        "        \n",
        "        if not tos['allowed']:\n",
        "            return False, \"Scraping not allowed\"\n",
        "        \n",
        "        if request_rate > tos['rate_limit']:\n",
        "            return False, f\"Rate limit exceeded: {tos['rate_limit']} requests/hour\"\n",
        "        \n",
        "        return True, \"Compliant\"\n",
        "```\n",
        "\n",
        "#### 2. **Data Privacy Compliance**\n",
        "\n",
        "```python\n",
        "class PrivacyCompliance:\n",
        "    \"\"\"Ensure privacy compliance in scraping\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.gdpr_requirements = {\n",
        "            'data_minimization': True,\n",
        "            'purpose_limitation': True,\n",
        "            'storage_limitation': True,\n",
        "            'accuracy': True,\n",
        "            'security': True,\n",
        "        }\n",
        "    \n",
        "    def anonymize_data(self, data):\n",
        "        \"\"\"Anonymize personal data in scraped content\"\"\"\n",
        "        # Remove email addresses\n",
        "        import re\n",
        "        data = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', data)\n",
        "        \n",
        "        # Remove phone numbers\n",
        "        data = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', data)\n",
        "        \n",
        "        # Remove IP addresses\n",
        "        data = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', '[IP]', data)\n",
        "        \n",
        "        return data\n",
        "    \n",
        "    def validate_data_usage(self, data, purpose):\n",
        "        \"\"\"Validate data usage against privacy requirements\"\"\"\n",
        "        # Check if data is necessary for purpose\n",
        "        if not self.is_data_necessary(data, purpose):\n",
        "            return False, \"Data not necessary for stated purpose\"\n",
        "        \n",
        "        # Check if data is minimized\n",
        "        if not self.is_data_minimized(data, purpose):\n",
        "            return False, \"Data not minimized\"\n",
        "        \n",
        "        return True, \"Privacy compliant\"\n",
        "```\n",
        "\n",
        "### Benefits for Our RAG Chatbot:\n",
        "\n",
        "1. **Sustainable Scraping**: Avoid getting blocked or banned\n",
        "2. **Ethical Compliance**: Respect website resources and terms\n",
        "3. **Reliable Data**: Consistent access to content\n",
        "4. **Legal Safety**: Comply with applicable laws and regulations\n",
        "5. **Resource Efficiency**: Optimize request patterns\n",
        "6. **Long-term Viability**: Maintain access over time\n",
        "7. **Reputation Management**: Build trust with content providers\n",
        "8. **Quality Assurance**: Ensure high-quality, reliable data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q4: How do we implement data cleaning and preprocessing for scraped content?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Data cleaning and preprocessing are essential steps in web scraping to ensure high-quality data for our RAG system. Our Greek Derby chatbot implements comprehensive data cleaning techniques to transform raw scraped content into clean, structured, and useful information.\n",
        "\n",
        "### Why Data Cleaning Matters:\n",
        "\n",
        "#### 1. **Common Data Quality Issues**\n",
        "\n",
        "```python\n",
        "# Typical issues in scraped data\n",
        "data_quality_issues = {\n",
        "    'html_entities': '&amp; &lt; &gt; &quot; &#39;',\n",
        "    'extra_whitespace': 'Multiple   spaces   and\\n\\n\\nnewlines',\n",
        "    'html_tags': '<p>Text</p><div>More text</div>',\n",
        "    'javascript_code': 'var x = 1; function() { return x; }',\n",
        "    'navigation_text': 'Home | About | Contact | Login',\n",
        "    'advertisement_text': 'Sponsored Content | Click Here',\n",
        "    'duplicate_content': 'Same text repeated multiple times',\n",
        "    'encoding_issues': 'Special characters not properly decoded',\n",
        "    'irrelevant_content': 'Comments, footers, headers',\n",
        "    'broken_text': 'Incomplete sentences or fragments'\n",
        "}\n",
        "```\n",
        "\n",
        "#### 2. **Impact on RAG System**\n",
        "\n",
        "**Poor Data Quality Effects:**\n",
        "- **Noise in Embeddings**: Irrelevant content affects vector similarity\n",
        "- **Reduced Accuracy**: Low-quality chunks lead to poor answers\n",
        "- **Storage Waste**: Unnecessary data consumes vector database space\n",
        "- **Performance Issues**: Processing irrelevant content slows down retrieval\n",
        "- **User Experience**: Poor quality responses from the chatbot\n",
        "\n",
        "### Our Data Cleaning Pipeline:\n",
        "\n",
        "#### 1. **HTML Content Extraction**\n",
        "\n",
        "```python\n",
        "import re\n",
        "import html\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class HTMLContentExtractor:\n",
        "    \"\"\"Extract clean text from HTML content\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.unwanted_tags = [\n",
        "            'script', 'style', 'nav', 'header', 'footer', \n",
        "            'aside', 'advertisement', 'ad', 'sidebar',\n",
        "            'menu', 'navigation', 'social', 'share',\n",
        "            'comment', 'comments', 'related', 'recommended'\n",
        "        ]\n",
        "        \n",
        "        self.unwanted_classes = [\n",
        "            'ad', 'advertisement', 'sidebar', 'navigation',\n",
        "            'menu', 'footer', 'header', 'social', 'share',\n",
        "            'comment', 'related', 'recommended', 'sponsored'\n",
        "        ]\n",
        "    \n",
        "    def extract_clean_text(self, html_content):\n",
        "        \"\"\"Extract clean text from HTML\"\"\"\n",
        "        \n",
        "        # Parse HTML\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        \n",
        "        # Remove unwanted tags\n",
        "        for tag in self.unwanted_tags:\n",
        "            for element in soup.find_all(tag):\n",
        "                element.decompose()\n",
        "        \n",
        "        # Remove elements with unwanted classes\n",
        "        for class_name in self.unwanted_classes:\n",
        "            for element in soup.find_all(class_=class_name):\n",
        "                element.decompose()\n",
        "        \n",
        "        # Get text content\n",
        "        text = soup.get_text()\n",
        "        \n",
        "        # Clean up text\n",
        "        text = self.clean_text(text)\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean extracted text\"\"\"\n",
        "        \n",
        "        # Decode HTML entities\n",
        "        text = html.unescape(text)\n",
        "        \n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        \n",
        "        # Remove leading/trailing whitespace\n",
        "        text = text.strip()\n",
        "        \n",
        "        # Remove common noise patterns\n",
        "        text = self.remove_noise_patterns(text)\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def remove_noise_patterns(self, text):\n",
        "        \"\"\"Remove common noise patterns\"\"\"\n",
        "        \n",
        "        # Remove navigation patterns\n",
        "        text = re.sub(r'Home\\s*\\|\\s*About\\s*\\|\\s*Contact', '', text)\n",
        "        text = re.sub(r'Login\\s*\\|\\s*Register\\s*\\|\\s*Sign\\s*Up', '', text)\n",
        "        \n",
        "        # Remove advertisement patterns\n",
        "        text = re.sub(r'Sponsored\\s*Content', '', text)\n",
        "        text = re.sub(r'Click\\s*Here\\s*Now', '', text)\n",
        "        text = re.sub(r'Ad\\s*Advertisement', '', text)\n",
        "        \n",
        "        # Remove social media patterns\n",
        "        text = re.sub(r'Follow\\s*us\\s*on\\s*Facebook|Twitter|Instagram', '', text)\n",
        "        text = re.sub(r'Share\\s*on\\s*Social\\s*Media', '', text)\n",
        "        \n",
        "        # Remove cookie notices\n",
        "        text = re.sub(r'This\\s*website\\s*uses\\s*cookies', '', text)\n",
        "        text = re.sub(r'Accept\\s*Cookies\\s*Privacy\\s*Policy', '', text)\n",
        "        \n",
        "        return text\n",
        "```\n",
        "\n",
        "#### 2. **Greek Language Specific Cleaning**\n",
        "\n",
        "```python\n",
        "class GreekTextCleaner:\n",
        "    \"\"\"Clean Greek text content specifically\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Greek-specific patterns\n",
        "        self.greek_patterns = {\n",
        "            'diacritics': r'[άέήίόύώΆΈΉΊΌΎΏ]',\n",
        "            'greek_chars': r'[α-ωΑ-Ω]',\n",
        "            'greek_punctuation': r'[·΄]',\n",
        "        }\n",
        "        \n",
        "        # Common Greek noise patterns\n",
        "        self.greek_noise = [\n",
        "            r'Σελίδα\\s*\\d+\\s*από\\s*\\d+',  # Page X of Y\n",
        "            r'Διαβάστε\\s*περισσότερα',    # Read more\n",
        "            r'Σχολιάστε\\s*αυτό\\s*το\\s*άρθρο',  # Comment on this article\n",
        "            r'Μοιραστείτε\\s*στο\\s*Facebook',   # Share on Facebook\n",
        "            r'Ακολουθήστε\\s*μας\\s*στο\\s*Twitter',  # Follow us on Twitter\n",
        "        ]\n",
        "    \n",
        "    def clean_greek_text(self, text):\n",
        "        \"\"\"Clean Greek text content\"\"\"\n",
        "        \n",
        "        # Remove Greek noise patterns\n",
        "        for pattern in self.greek_noise:\n",
        "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "        \n",
        "        # Normalize Greek diacritics\n",
        "        text = self.normalize_greek_diacritics(text)\n",
        "        \n",
        "        # Remove extra punctuation\n",
        "        text = re.sub(r'[·΄]+', '', text)\n",
        "        \n",
        "        # Clean up spacing\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def normalize_greek_diacritics(self, text):\n",
        "        \"\"\"Normalize Greek diacritics for consistency\"\"\"\n",
        "        \n",
        "        # Common diacritic mappings\n",
        "        diacritic_map = {\n",
        "            'ά': 'α', 'έ': 'ε', 'ή': 'η', 'ί': 'ι',\n",
        "            'ό': 'ο', 'ύ': 'υ', 'ώ': 'ω',\n",
        "            'Ά': 'Α', 'Έ': 'Ε', 'Ή': 'Η', 'Ί': 'Ι',\n",
        "            'Ό': 'Ο', 'Ύ': 'Υ', 'Ώ': 'Ω'\n",
        "        }\n",
        "        \n",
        "        for diacritic, base in diacritic_map.items():\n",
        "            text = text.replace(diacritic, base)\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def is_greek_content(self, text):\n",
        "        \"\"\"Check if text is primarily Greek\"\"\"\n",
        "        \n",
        "        greek_chars = len(re.findall(self.greek_patterns['greek_chars'], text))\n",
        "        total_chars = len(re.sub(r'\\s', '', text))\n",
        "        \n",
        "        if total_chars == 0:\n",
        "            return False\n",
        "        \n",
        "        greek_ratio = greek_chars / total_chars\n",
        "        return greek_ratio > 0.3  # 30% Greek content threshold\n",
        "```\n",
        "\n",
        "#### 3. **Content Quality Assessment**\n",
        "\n",
        "```python\n",
        "class ContentQualityAssessor:\n",
        "    \"\"\"Assess and filter content quality\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.min_length = 50\n",
        "        self.max_length = 10000\n",
        "        self.min_word_count = 10\n",
        "        self.max_link_density = 0.3\n",
        "        self.min_sentence_count = 2\n",
        "    \n",
        "    def assess_quality(self, text, metadata=None):\n",
        "        \"\"\"Assess content quality and return score\"\"\"\n",
        "        \n",
        "        if not text or not text.strip():\n",
        "            return 0, \"Empty content\"\n",
        "        \n",
        "        # Basic metrics\n",
        "        length = len(text.strip())\n",
        "        word_count = len(text.split())\n",
        "        sentence_count = len(re.findall(r'[.!?]+', text))\n",
        "        \n",
        "        # Link density\n",
        "        link_count = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
        "        link_density = link_count / max(word_count, 1)\n",
        "        \n",
        "        # Quality checks\n",
        "        checks = {\n",
        "            'length_ok': self.min_length <= length <= self.max_length,\n",
        "            'word_count_ok': word_count >= self.min_word_count,\n",
        "            'sentence_count_ok': sentence_count >= self.min_sentence_count,\n",
        "            'link_density_ok': link_density <= self.max_link_density,\n",
        "            'not_navigation': not self.is_navigation_text(text),\n",
        "            'not_advertisement': not self.is_advertisement_text(text),\n",
        "            'has_meaningful_content': self.has_meaningful_content(text),\n",
        "        }\n",
        "        \n",
        "        # Calculate quality score\n",
        "        quality_score = sum(checks.values()) / len(checks)\n",
        "        \n",
        "        # Determine quality level\n",
        "        if quality_score >= 0.8:\n",
        "            quality_level = \"High\"\n",
        "        elif quality_score >= 0.6:\n",
        "            quality_level = \"Medium\"\n",
        "        else:\n",
        "            quality_level = \"Low\"\n",
        "        \n",
        "        return quality_score, quality_level, checks\n",
        "    \n",
        "    def is_navigation_text(self, text):\n",
        "        \"\"\"Check if text is navigation content\"\"\"\n",
        "        nav_patterns = [\n",
        "            r'Home\\s*\\|\\s*About\\s*\\|\\s*Contact',\n",
        "            r'Login\\s*\\|\\s*Register',\n",
        "            r'Menu\\s*\\|\\s*Navigation',\n",
        "            r'Σελίδα\\s*\\d+\\s*από\\s*\\d+',  # Greek: Page X of Y\n",
        "        ]\n",
        "        \n",
        "        for pattern in nav_patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def is_advertisement_text(self, text):\n",
        "        \"\"\"Check if text is advertisement content\"\"\"\n",
        "        ad_patterns = [\n",
        "            r'Sponsored\\s*Content',\n",
        "            r'Advertisement',\n",
        "            r'Click\\s*Here\\s*Now',\n",
        "            r'Buy\\s*Now',\n",
        "            r'Limited\\s*Time\\s*Offer',\n",
        "            r'Διαφήμιση',  # Greek: Advertisement\n",
        "            r'Προσφορά',  # Greek: Offer\n",
        "        ]\n",
        "        \n",
        "        for pattern in ad_patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def has_meaningful_content(self, text):\n",
        "        \"\"\"Check if text has meaningful content\"\"\"\n",
        "        \n",
        "        # Check for meaningful words (not just common words)\n",
        "        meaningful_words = [\n",
        "            'match', 'game', 'team', 'player', 'coach', 'goal', 'score',\n",
        "            'άγων', 'ομάδα', 'παίκτης', 'προπονητής', 'γκολ', 'σκορ',  # Greek\n",
        "            'football', 'soccer', 'championship', 'league', 'derby',\n",
        "            'ποδόσφαιρο', 'πρωτάθλημα', 'λίγκα', 'ντέρμπι',  # Greek\n",
        "        ]\n",
        "        \n",
        "        text_lower = text.lower()\n",
        "        meaningful_count = sum(1 for word in meaningful_words if word in text_lower)\n",
        "        \n",
        "        return meaningful_count >= 2\n",
        "```\n",
        "\n",
        "### Advanced Data Processing:\n",
        "\n",
        "#### 1. **Content Deduplication**\n",
        "\n",
        "```python\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "\n",
        "class ContentDeduplicator:\n",
        "    \"\"\"Remove duplicate content from scraped data\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.content_hashes = set()\n",
        "        self.similarity_threshold = 0.8\n",
        "    \n",
        "    def deduplicate_documents(self, documents):\n",
        "        \"\"\"Remove duplicate documents\"\"\"\n",
        "        \n",
        "        unique_docs = []\n",
        "        seen_hashes = set()\n",
        "        \n",
        "        for doc in documents:\n",
        "            # Create content hash\n",
        "            content_hash = self.create_content_hash(doc.page_content)\n",
        "            \n",
        "            if content_hash not in seen_hashes:\n",
        "                seen_hashes.add(content_hash)\n",
        "                unique_docs.append(doc)\n",
        "            else:\n",
        "                print(f\"Duplicate document found: {doc.metadata.get('source', 'Unknown')}\")\n",
        "        \n",
        "        return unique_docs\n",
        "    \n",
        "    def create_content_hash(self, content):\n",
        "        \"\"\"Create hash for content deduplication\"\"\"\n",
        "        \n",
        "        # Normalize content\n",
        "        normalized = re.sub(r'\\s+', ' ', content.strip().lower())\n",
        "        \n",
        "        # Create hash\n",
        "        return hashlib.md5(normalized.encode()).hexdigest()\n",
        "    \n",
        "    def find_similar_content(self, documents):\n",
        "        \"\"\"Find similar content using text similarity\"\"\"\n",
        "        \n",
        "        from difflib import SequenceMatcher\n",
        "        \n",
        "        similar_groups = defaultdict(list)\n",
        "        \n",
        "        for i, doc1 in enumerate(documents):\n",
        "            for j, doc2 in enumerate(documents[i+1:], i+1):\n",
        "                similarity = SequenceMatcher(None, doc1.page_content, doc2.page_content).ratio()\n",
        "                \n",
        "                if similarity >= self.similarity_threshold:\n",
        "                    similar_groups[i].append((j, similarity))\n",
        "        \n",
        "        return similar_groups\n",
        "```\n",
        "\n",
        "#### 2. **Content Enrichment**\n",
        "\n",
        "```python\n",
        "class ContentEnricher:\n",
        "    \"\"\"Enrich scraped content with metadata and context\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.greek_football_terms = {\n",
        "            'teams': ['Ολυμπιακός', 'Παναθηναϊκός', 'ΑΕΚ', 'ΠΑΟΚ'],\n",
        "            'competitions': ['Σούπερ Λίγκα', 'Champions League', 'Europa League'],\n",
        "            'positions': ['επιθετικός', 'μέσος', 'αμυντικός', 'τερματοφύλακας'],\n",
        "        }\n",
        "    \n",
        "    def enrich_document(self, doc, source_url):\n",
        "        \"\"\"Enrich document with metadata and context\"\"\"\n",
        "        \n",
        "        # Extract metadata from URL\n",
        "        metadata = self.extract_url_metadata(source_url)\n",
        "        \n",
        "        # Detect content type\n",
        "        content_type = self.detect_content_type(doc.page_content)\n",
        "        metadata['content_type'] = content_type\n",
        "        \n",
        "        # Extract entities\n",
        "        entities = self.extract_entities(doc.page_content)\n",
        "        metadata['entities'] = entities\n",
        "        \n",
        "        # Detect language\n",
        "        language = self.detect_language(doc.page_content)\n",
        "        metadata['language'] = language\n",
        "        \n",
        "        # Add timestamp\n",
        "        metadata['processed_at'] = datetime.now().isoformat()\n",
        "        \n",
        "        # Update document metadata\n",
        "        doc.metadata.update(metadata)\n",
        "        \n",
        "        return doc\n",
        "    \n",
        "    def extract_url_metadata(self, url):\n",
        "        \"\"\"Extract metadata from URL\"\"\"\n",
        "        \n",
        "        from urllib.parse import urlparse\n",
        "        \n",
        "        parsed = urlparse(url)\n",
        "        \n",
        "        metadata = {\n",
        "            'domain': parsed.netloc,\n",
        "            'path': parsed.path,\n",
        "            'source': url,\n",
        "        }\n",
        "        \n",
        "        # Extract section from path\n",
        "        path_parts = parsed.path.strip('/').split('/')\n",
        "        if path_parts:\n",
        "            metadata['section'] = path_parts[0]\n",
        "        \n",
        "        return metadata\n",
        "    \n",
        "    def detect_content_type(self, content):\n",
        "        \"\"\"Detect type of content\"\"\"\n",
        "        \n",
        "        content_lower = content.lower()\n",
        "        \n",
        "        if any(word in content_lower for word in ['match', 'game', 'άγων', 'αγώνας']):\n",
        "            return 'match_report'\n",
        "        elif any(word in content_lower for word in ['transfer', 'μεταγραφή', 'μεταγραφές']):\n",
        "            return 'transfer_news'\n",
        "        elif any(word in content_lower for word in ['injury', 'τραυματισμός', 'τραυματισμοί']):\n",
        "            return 'injury_news'\n",
        "        elif any(word in content_lower for word in ['preview', 'προεπισκόπηση', 'προβλέψεις']):\n",
        "            return 'match_preview'\n",
        "        else:\n",
        "            return 'general_news'\n",
        "    \n",
        "    def extract_entities(self, content):\n",
        "        \"\"\"Extract relevant entities from content\"\"\"\n",
        "        \n",
        "        entities = {\n",
        "            'teams': [],\n",
        "            'players': [],\n",
        "            'competitions': [],\n",
        "            'dates': [],\n",
        "        }\n",
        "        \n",
        "        # Extract teams\n",
        "        for team in self.greek_football_terms['teams']:\n",
        "            if team in content:\n",
        "                entities['teams'].append(team)\n",
        "        \n",
        "        # Extract competitions\n",
        "        for comp in self.greek_football_terms['competitions']:\n",
        "            if comp in content:\n",
        "                entities['competitions'].append(comp)\n",
        "        \n",
        "        # Extract dates (simple pattern)\n",
        "        date_pattern = r'\\d{1,2}/\\d{1,2}/\\d{4}|\\d{4}-\\d{2}-\\d{2}'\n",
        "        entities['dates'] = re.findall(date_pattern, content)\n",
        "        \n",
        "        return entities\n",
        "    \n",
        "    def detect_language(self, content):\n",
        "        \"\"\"Detect primary language of content\"\"\"\n",
        "        \n",
        "        greek_chars = len(re.findall(r'[α-ωΑ-Ω]', content))\n",
        "        english_chars = len(re.findall(r'[a-zA-Z]', content))\n",
        "        \n",
        "        total_chars = greek_chars + english_chars\n",
        "        \n",
        "        if total_chars == 0:\n",
        "            return 'unknown'\n",
        "        \n",
        "        greek_ratio = greek_chars / total_chars\n",
        "        \n",
        "        if greek_ratio > 0.5:\n",
        "            return 'greek'\n",
        "        elif greek_ratio > 0.1:\n",
        "            return 'mixed'\n",
        "        else:\n",
        "            return 'english'\n",
        "```\n",
        "\n",
        "### Integration with RAG System:\n",
        "\n",
        "#### 1. **Preprocessing for Vector Database**\n",
        "\n",
        "```python\n",
        "class RAGPreprocessor:\n",
        "    \"\"\"Preprocess content for RAG system\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.html_extractor = HTMLContentExtractor()\n",
        "        self.greek_cleaner = GreekTextCleaner()\n",
        "        self.quality_assessor = ContentQualityAssessor()\n",
        "        self.deduplicator = ContentDeduplicator()\n",
        "        self.enricher = ContentEnricher()\n",
        "    \n",
        "    def preprocess_documents(self, raw_documents):\n",
        "        \"\"\"Preprocess documents for RAG system\"\"\"\n",
        "        \n",
        "        processed_docs = []\n",
        "        \n",
        "        for doc in raw_documents:\n",
        "            try:\n",
        "                # Extract clean text\n",
        "                clean_text = self.html_extractor.extract_clean_text(doc.page_content)\n",
        "                \n",
        "                # Clean Greek text\n",
        "                clean_text = self.greek_cleaner.clean_greek_text(clean_text)\n",
        "                \n",
        "                # Assess quality\n",
        "                quality_score, quality_level, checks = self.quality_assessor.assess_quality(clean_text)\n",
        "                \n",
        "                # Skip low-quality content\n",
        "                if quality_score < 0.6:\n",
        "                    print(f\"Skipping low-quality content: {quality_level}\")\n",
        "                    continue\n",
        "                \n",
        "                # Update document content\n",
        "                doc.page_content = clean_text\n",
        "                \n",
        "                # Enrich with metadata\n",
        "                doc = self.enricher.enrich_document(doc, doc.metadata.get('source', ''))\n",
        "                \n",
        "                # Add quality metadata\n",
        "                doc.metadata.update({\n",
        "                    'quality_score': quality_score,\n",
        "                    'quality_level': quality_level,\n",
        "                    'quality_checks': checks\n",
        "                })\n",
        "                \n",
        "                processed_docs.append(doc)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Remove duplicates\n",
        "        processed_docs = self.deduplicator.deduplicate_documents(processed_docs)\n",
        "        \n",
        "        return processed_docs\n",
        "```\n",
        "\n",
        "### Benefits for Our RAG Chatbot:\n",
        "\n",
        "1. **High-Quality Data**: Clean, relevant content for better embeddings\n",
        "2. **Language Accuracy**: Proper Greek text processing and normalization\n",
        "3. **Noise Reduction**: Remove irrelevant content and advertisements\n",
        "4. **Deduplication**: Avoid duplicate content in the knowledge base\n",
        "5. **Metadata Enrichment**: Rich context for better retrieval\n",
        "6. **Quality Assurance**: Ensure only high-quality content is stored\n",
        "7. **Performance**: Optimized content for faster processing\n",
        "8. **Scalability**: Efficient preprocessing for large amounts of data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q5: How do we implement error handling and resilience in web scraping?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Error handling and resilience are critical for maintaining a robust web scraping system. Our Greek Derby chatbot implements comprehensive error handling strategies to ensure continuous operation even when facing network issues, website changes, or anti-bot measures.\n",
        "\n",
        "### Common Scraping Challenges:\n",
        "\n",
        "#### 1. **Network and Connection Issues**\n",
        "\n",
        "```python\n",
        "# Common network errors in web scraping\n",
        "network_errors = {\n",
        "    'ConnectionError': 'Cannot connect to the server',\n",
        "    'TimeoutError': 'Request timed out',\n",
        "    'DNSResolutionError': 'Cannot resolve domain name',\n",
        "    'SSLHandshakeError': 'SSL/TLS handshake failed',\n",
        "    'ProxyError': 'Proxy server connection failed',\n",
        "    'TooManyRedirects': 'Too many redirects',\n",
        "    'ChunkedEncodingError': 'Chunked encoding error',\n",
        "    'ReadTimeoutError': 'Read operation timed out',\n",
        "}\n",
        "```\n",
        "\n",
        "#### 2. **HTTP Status Code Errors**\n",
        "\n",
        "```python\n",
        "# HTTP status codes and their meanings\n",
        "http_status_codes = {\n",
        "    200: 'OK - Request successful',\n",
        "    301: 'Moved Permanently - Redirect',\n",
        "    302: 'Found - Temporary redirect',\n",
        "    403: 'Forbidden - Access denied',\n",
        "    404: 'Not Found - Page does not exist',\n",
        "    429: 'Too Many Requests - Rate limited',\n",
        "    500: 'Internal Server Error - Server error',\n",
        "    502: 'Bad Gateway - Proxy error',\n",
        "    503: 'Service Unavailable - Server overloaded',\n",
        "    504: 'Gateway Timeout - Proxy timeout',\n",
        "}\n",
        "```\n",
        "\n",
        "### Our Error Handling Strategy:\n",
        "\n",
        "#### 1. **Comprehensive Exception Handling**\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from requests.exceptions import (\n",
        "    RequestException, ConnectionError, Timeout, \n",
        "    TooManyRedirects, HTTPError, ProxyError\n",
        ")\n",
        "import time\n",
        "import random\n",
        "from functools import wraps\n",
        "\n",
        "class ScrapingErrorHandler:\n",
        "    \"\"\"Handle errors in web scraping operations\"\"\"\n",
        "    \n",
        "    def __init__(self, max_retries=3, base_delay=1, max_delay=60):\n",
        "        self.max_retries = max_retries\n",
        "        self.base_delay = base_delay\n",
        "        self.max_delay = max_delay\n",
        "        self.error_counts = {}\n",
        "    \n",
        "    def handle_request_errors(self, func):\n",
        "        \"\"\"Decorator to handle request errors with retry logic\"\"\"\n",
        "        \n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            last_exception = None\n",
        "            \n",
        "            for attempt in range(self.max_retries + 1):\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                    \n",
        "                except ConnectionError as e:\n",
        "                    last_exception = e\n",
        "                    self.log_error('ConnectionError', str(e), attempt)\n",
        "                    if attempt < self.max_retries:\n",
        "                        self.wait_with_backoff(attempt)\n",
        "                        continue\n",
        "                    \n",
        "                except Timeout as e:\n",
        "                    last_exception = e\n",
        "                    self.log_error('Timeout', str(e), attempt)\n",
        "                    if attempt < self.max_retries:\n",
        "                        self.wait_with_backoff(attempt)\n",
        "                        continue\n",
        "                    \n",
        "                except HTTPError as e:\n",
        "                    last_exception = e\n",
        "                    status_code = e.response.status_code\n",
        "                    self.log_error('HTTPError', f\"Status {status_code}: {str(e)}\", attempt)\n",
        "                    \n",
        "                    # Don't retry certain status codes\n",
        "                    if status_code in [403, 404, 401]:\n",
        "                        break\n",
        "                    \n",
        "                    if attempt < self.max_retries:\n",
        "                        self.wait_with_backoff(attempt)\n",
        "                        continue\n",
        "                    \n",
        "                except TooManyRedirects as e:\n",
        "                    last_exception = e\n",
        "                    self.log_error('TooManyRedirects', str(e), attempt)\n",
        "                    if attempt < self.max_retries:\n",
        "                        self.wait_with_backoff(attempt)\n",
        "                        continue\n",
        "                    \n",
        "                except ProxyError as e:\n",
        "                    last_exception = e\n",
        "                    self.log_error('ProxyError', str(e), attempt)\n",
        "                    if attempt < self.max_retries:\n",
        "                        self.wait_with_backoff(attempt)\n",
        "                        continue\n",
        "                    \n",
        "                except RequestException as e:\n",
        "                    last_exception = e\n",
        "                    self.log_error('RequestException', str(e), attempt)\n",
        "                    if attempt < self.max_retries:\n",
        "                        self.wait_with_backoff(attempt)\n",
        "                        continue\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    last_exception = e\n",
        "                    self.log_error('UnexpectedError', str(e), attempt)\n",
        "                    break\n",
        "            \n",
        "            # If all retries failed, raise the last exception\n",
        "            raise last_exception\n",
        "        \n",
        "        return wrapper\n",
        "    \n",
        "    def wait_with_backoff(self, attempt):\n",
        "        \"\"\"Wait with exponential backoff and jitter\"\"\"\n",
        "        \n",
        "        # Exponential backoff with jitter\n",
        "        delay = min(\n",
        "            self.base_delay * (2 ** attempt) + random.uniform(0, 1),\n",
        "            self.max_delay\n",
        "        )\n",
        "        \n",
        "        print(f\"Waiting {delay:.2f} seconds before retry {attempt + 1}\")\n",
        "        time.sleep(delay)\n",
        "    \n",
        "    def log_error(self, error_type, message, attempt):\n",
        "        \"\"\"Log error information\"\"\"\n",
        "        \n",
        "        # Track error counts\n",
        "        if error_type not in self.error_counts:\n",
        "            self.error_counts[error_type] = 0\n",
        "        self.error_counts[error_type] += 1\n",
        "        \n",
        "        print(f\"[{error_type}] Attempt {attempt + 1}: {message}\")\n",
        "    \n",
        "    def get_error_stats(self):\n",
        "        \"\"\"Get error statistics\"\"\"\n",
        "        return self.error_counts.copy()\n",
        "```\n",
        "\n",
        "#### 2. **Robust Request Implementation**\n",
        "\n",
        "```python\n",
        "class RobustScraper:\n",
        "    \"\"\"Implement robust scraping with comprehensive error handling\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.error_handler = ScrapingErrorHandler()\n",
        "        self.session = self.setup_session()\n",
        "    \n",
        "    def setup_session(self):\n",
        "        \"\"\"Setup requests session with proper configuration\"\"\"\n",
        "        \n",
        "        session = requests.Session()\n",
        "        \n",
        "        # Configure retry strategy\n",
        "        from requests.adapters import HTTPAdapter\n",
        "        from urllib3.util.retry import Retry\n",
        "        \n",
        "        retry_strategy = Retry(\n",
        "            total=3,\n",
        "            backoff_factor=1,\n",
        "            status_forcelist=[429, 500, 502, 503, 504],\n",
        "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
        "        )\n",
        "        \n",
        "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "        session.mount(\"http://\", adapter)\n",
        "        session.mount(\"https://\", adapter)\n",
        "        \n",
        "        # Set default headers\n",
        "        session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "        })\n",
        "        \n",
        "        return session\n",
        "    \n",
        "    @error_handler.handle_request_errors\n",
        "    def make_request(self, url, **kwargs):\n",
        "        \"\"\"Make HTTP request with error handling\"\"\"\n",
        "        \n",
        "        # Set default timeout\n",
        "        if 'timeout' not in kwargs:\n",
        "            kwargs['timeout'] = 30\n",
        "        \n",
        "        # Make request\n",
        "        response = self.session.get(url, **kwargs)\n",
        "        \n",
        "        # Check for HTTP errors\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    def scrape_url_safely(self, url):\n",
        "        \"\"\"Scrape URL with comprehensive error handling\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = self.make_request(url)\n",
        "            \n",
        "            # Check content type\n",
        "            content_type = response.headers.get('content-type', '')\n",
        "            if 'text/html' not in content_type:\n",
        "                print(f\"Warning: Non-HTML content type: {content_type}\")\n",
        "            \n",
        "            # Check content length\n",
        "            content_length = len(response.content)\n",
        "            if content_length < 100:\n",
        "                print(f\"Warning: Very short content ({content_length} bytes)\")\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {url}: {e}\")\n",
        "            return None\n",
        "```\n",
        "\n",
        "### Advanced Resilience Patterns:\n",
        "\n",
        "#### 1. **Circuit Breaker Pattern**\n",
        "\n",
        "```python\n",
        "import time\n",
        "from enum import Enum\n",
        "\n",
        "class CircuitState(Enum):\n",
        "    CLOSED = \"closed\"\n",
        "    OPEN = \"open\"\n",
        "    HALF_OPEN = \"half_open\"\n",
        "\n",
        "class CircuitBreaker:\n",
        "    \"\"\"Implement circuit breaker pattern for scraping\"\"\"\n",
        "    \n",
        "    def __init__(self, failure_threshold=5, recovery_timeout=60, expected_exception=Exception):\n",
        "        self.failure_threshold = failure_threshold\n",
        "        self.recovery_timeout = recovery_timeout\n",
        "        self.expected_exception = expected_exception\n",
        "        \n",
        "        self.failure_count = 0\n",
        "        self.last_failure_time = None\n",
        "        self.state = CircuitState.CLOSED\n",
        "    \n",
        "    def call(self, func, *args, **kwargs):\n",
        "        \"\"\"Execute function with circuit breaker protection\"\"\"\n",
        "        \n",
        "        if self.state == CircuitState.OPEN:\n",
        "            if self._should_attempt_reset():\n",
        "                self.state = CircuitState.HALF_OPEN\n",
        "            else:\n",
        "                raise Exception(\"Circuit breaker is OPEN\")\n",
        "        \n",
        "        try:\n",
        "            result = func(*args, **kwargs)\n",
        "            self._on_success()\n",
        "            return result\n",
        "            \n",
        "        except self.expected_exception as e:\n",
        "            self._on_failure()\n",
        "            raise e\n",
        "    \n",
        "    def _should_attempt_reset(self):\n",
        "        \"\"\"Check if circuit breaker should attempt reset\"\"\"\n",
        "        return (\n",
        "            time.time() - self.last_failure_time >= self.recovery_timeout\n",
        "        )\n",
        "    \n",
        "    def _on_success(self):\n",
        "        \"\"\"Handle successful call\"\"\"\n",
        "        self.failure_count = 0\n",
        "        self.state = CircuitState.CLOSED\n",
        "    \n",
        "    def _on_failure(self):\n",
        "        \"\"\"Handle failed call\"\"\"\n",
        "        self.failure_count += 1\n",
        "        self.last_failure_time = time.time()\n",
        "        \n",
        "        if self.failure_count >= self.failure_threshold:\n",
        "            self.state = CircuitState.OPEN\n",
        "\n",
        "# Usage with circuit breaker\n",
        "circuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n",
        "\n",
        "def scrape_with_circuit_breaker(url):\n",
        "    \"\"\"Scrape URL with circuit breaker protection\"\"\"\n",
        "    return circuit_breaker.call(make_request, url)\n",
        "```\n",
        "\n",
        "#### 2. **Fallback Strategies**\n",
        "\n",
        "```python\n",
        "class FallbackScraper:\n",
        "    \"\"\"Implement fallback strategies for scraping\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.primary_scraper = RobustScraper()\n",
        "        self.fallback_scrapers = [\n",
        "            self._scrape_with_selenium,\n",
        "            self._scrape_with_requests_alternative,\n",
        "            self._scrape_with_cached_content,\n",
        "        ]\n",
        "    \n",
        "    def scrape_with_fallbacks(self, url):\n",
        "        \"\"\"Scrape URL with multiple fallback strategies\"\"\"\n",
        "        \n",
        "        # Try primary scraper first\n",
        "        try:\n",
        "            result = self.primary_scraper.scrape_url_safely(url)\n",
        "            if result and result.status_code == 200:\n",
        "                return result\n",
        "        except Exception as e:\n",
        "            print(f\"Primary scraper failed: {e}\")\n",
        "        \n",
        "        # Try fallback strategies\n",
        "        for i, fallback_scraper in enumerate(self.fallback_scrapers):\n",
        "            try:\n",
        "                print(f\"Trying fallback strategy {i + 1}\")\n",
        "                result = fallback_scraper(url)\n",
        "                if result:\n",
        "                    return result\n",
        "            except Exception as e:\n",
        "                print(f\"Fallback strategy {i + 1} failed: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(\"All scraping strategies failed\")\n",
        "        return None\n",
        "    \n",
        "    def _scrape_with_selenium(self, url):\n",
        "        \"\"\"Fallback: Use Selenium for JavaScript-heavy sites\"\"\"\n",
        "        try:\n",
        "            from selenium import webdriver\n",
        "            from selenium.webdriver.chrome.options import Options\n",
        "            \n",
        "            options = Options()\n",
        "            options.add_argument('--headless')\n",
        "            options.add_argument('--no-sandbox')\n",
        "            options.add_argument('--disable-dev-shm-usage')\n",
        "            \n",
        "            driver = webdriver.Chrome(options=options)\n",
        "            driver.get(url)\n",
        "            \n",
        "            # Wait for content to load\n",
        "            time.sleep(3)\n",
        "            \n",
        "            page_source = driver.page_source\n",
        "            driver.quit()\n",
        "            \n",
        "            # Create mock response object\n",
        "            class MockResponse:\n",
        "                def __init__(self, content):\n",
        "                    self.content = content.encode()\n",
        "                    self.status_code = 200\n",
        "                    self.headers = {'content-type': 'text/html'}\n",
        "            \n",
        "            return MockResponse(page_source)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Selenium fallback failed: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def _scrape_with_requests_alternative(self, url):\n",
        "        \"\"\"Fallback: Use alternative request configuration\"\"\"\n",
        "        try:\n",
        "            # Try with different user agent\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15'\n",
        "            }\n",
        "            \n",
        "            response = requests.get(url, headers=headers, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Alternative requests failed: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def _scrape_with_cached_content(self, url):\n",
        "        \"\"\"Fallback: Use cached content if available\"\"\"\n",
        "        try:\n",
        "            # Check if we have cached content\n",
        "            cache_key = f\"cached_{hash(url)}\"\n",
        "            # This would be implemented with actual caching logic\n",
        "            print(\"No cached content available\")\n",
        "            return None\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Cache fallback failed: {e}\")\n",
        "            return None\n",
        "```\n",
        "\n",
        "#### 3. **Monitoring and Alerting**\n",
        "\n",
        "```python\n",
        "class ScrapingMonitor:\n",
        "    \"\"\"Monitor scraping operations and alert on issues\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            'total_requests': 0,\n",
        "            'successful_requests': 0,\n",
        "            'failed_requests': 0,\n",
        "            'error_types': {},\n",
        "            'response_times': [],\n",
        "            'last_successful_request': None,\n",
        "            'last_failed_request': None,\n",
        "        }\n",
        "        \n",
        "        self.alert_thresholds = {\n",
        "            'failure_rate': 0.3,  # 30% failure rate\n",
        "            'max_response_time': 30,  # 30 seconds\n",
        "            'consecutive_failures': 5,\n",
        "        }\n",
        "    \n",
        "    def record_request(self, success, error_type=None, response_time=None):\n",
        "        \"\"\"Record request metrics\"\"\"\n",
        "        \n",
        "        self.metrics['total_requests'] += 1\n",
        "        \n",
        "        if success:\n",
        "            self.metrics['successful_requests'] += 1\n",
        "            self.metrics['last_successful_request'] = time.time()\n",
        "        else:\n",
        "            self.metrics['failed_requests'] += 1\n",
        "            self.metrics['last_failed_request'] = time.time()\n",
        "            \n",
        "            if error_type:\n",
        "                if error_type not in self.metrics['error_types']:\n",
        "                    self.metrics['error_types'][error_type] = 0\n",
        "                self.metrics['error_types'][error_type] += 1\n",
        "        \n",
        "        if response_time:\n",
        "            self.metrics['response_times'].append(response_time)\n",
        "            # Keep only last 100 response times\n",
        "            if len(self.metrics['response_times']) > 100:\n",
        "                self.metrics['response_times'] = self.metrics['response_times'][-100:]\n",
        "        \n",
        "        # Check for alerts\n",
        "        self.check_alerts()\n",
        "    \n",
        "    def check_alerts(self):\n",
        "        \"\"\"Check if any alerts should be triggered\"\"\"\n",
        "        \n",
        "        total = self.metrics['total_requests']\n",
        "        if total == 0:\n",
        "            return\n",
        "        \n",
        "        # Check failure rate\n",
        "        failure_rate = self.metrics['failed_requests'] / total\n",
        "        if failure_rate > self.alert_thresholds['failure_rate']:\n",
        "            self.trigger_alert(f\"High failure rate: {failure_rate:.2%}\")\n",
        "        \n",
        "        # Check response time\n",
        "        if self.metrics['response_times']:\n",
        "            avg_response_time = sum(self.metrics['response_times']) / len(self.metrics['response_times'])\n",
        "            if avg_response_time > self.alert_thresholds['max_response_time']:\n",
        "                self.trigger_alert(f\"High response time: {avg_response_time:.2f}s\")\n",
        "        \n",
        "        # Check consecutive failures\n",
        "        if self.metrics['last_failed_request'] and self.metrics['last_successful_request']:\n",
        "            if self.metrics['last_failed_request'] > self.metrics['last_successful_request']:\n",
        "                # Count consecutive failures\n",
        "                consecutive_failures = self.metrics['failed_requests'] - self.metrics['successful_requests']\n",
        "                if consecutive_failures >= self.alert_thresholds['consecutive_failures']:\n",
        "                    self.trigger_alert(f\"Consecutive failures: {consecutive_failures}\")\n",
        "    \n",
        "    def trigger_alert(self, message):\n",
        "        \"\"\"Trigger alert for monitoring issues\"\"\"\n",
        "        print(f\"ALERT: {message}\")\n",
        "        # In production, this would send to monitoring system\n",
        "        # send_to_monitoring_system(message)\n",
        "    \n",
        "    def get_metrics(self):\n",
        "        \"\"\"Get current metrics\"\"\"\n",
        "        return self.metrics.copy()\n",
        "```\n",
        "\n",
        "### Recovery and Self-Healing:\n",
        "\n",
        "#### 1. **Automatic Recovery**\n",
        "\n",
        "```python\n",
        "class SelfHealingScraper:\n",
        "    \"\"\"Implement self-healing scraping system\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.monitor = ScrapingMonitor()\n",
        "        self.circuit_breaker = CircuitBreaker()\n",
        "        self.fallback_scraper = FallbackScraper()\n",
        "        self.health_check_interval = 300  # 5 minutes\n",
        "        self.last_health_check = 0\n",
        "    \n",
        "    def scrape_with_self_healing(self, url):\n",
        "        \"\"\"Scrape URL with self-healing capabilities\"\"\"\n",
        "        \n",
        "        # Check if health check is needed\n",
        "        if time.time() - self.last_health_check > self.health_check_interval:\n",
        "            self.perform_health_check()\n",
        "            self.last_health_check = time.time()\n",
        "        \n",
        "        # Try scraping with circuit breaker\n",
        "        try:\n",
        "            result = self.circuit_breaker.call(\n",
        "                self.fallback_scraper.scrape_with_fallbacks, url\n",
        "            )\n",
        "            \n",
        "            if result:\n",
        "                self.monitor.record_request(True)\n",
        "                return result\n",
        "            else:\n",
        "                self.monitor.record_request(False, \"No result\")\n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            self.monitor.record_request(False, str(type(e).__name__))\n",
        "            return None\n",
        "    \n",
        "    def perform_health_check(self):\n",
        "        \"\"\"Perform health check on scraping system\"\"\"\n",
        "        \n",
        "        print(\"Performing health check...\")\n",
        "        \n",
        "        # Test with a simple URL\n",
        "        test_url = \"https://httpbin.org/status/200\"\n",
        "        \n",
        "        try:\n",
        "            response = requests.get(test_url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(\"Health check passed\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Health check failed: Status {response.status_code}\")\n",
        "                return False\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Health check failed: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def get_system_status(self):\n",
        "        \"\"\"Get current system status\"\"\"\n",
        "        \n",
        "        metrics = self.monitor.get_metrics()\n",
        "        circuit_state = self.circuit_breaker.state\n",
        "        \n",
        "        status = {\n",
        "            'circuit_breaker_state': circuit_state.value,\n",
        "            'metrics': metrics,\n",
        "            'health_status': 'healthy' if self.perform_health_check() else 'unhealthy'\n",
        "        }\n",
        "        \n",
        "        return status\n",
        "```\n",
        "\n",
        "### Benefits for Our RAG Chatbot:\n",
        "\n",
        "1. **Continuous Operation**: System continues working despite errors\n",
        "2. **Automatic Recovery**: Self-healing capabilities reduce manual intervention\n",
        "3. **Quality Assurance**: Error handling ensures data quality\n",
        "4. **Monitoring**: Real-time monitoring of scraping health\n",
        "5. **Fallback Strategies**: Multiple approaches ensure data availability\n",
        "6. **Performance**: Circuit breakers prevent cascading failures\n",
        "7. **Reliability**: Robust error handling increases system reliability\n",
        "8. **Maintainability**: Clear error logging and monitoring for debugging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q6: How do we integrate web scraping with our RAG system for optimal performance?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Integrating web scraping with our RAG system requires careful orchestration to ensure optimal performance, data quality, and system reliability. Our Greek Derby chatbot implements a sophisticated integration pipeline that transforms scraped content into high-quality embeddings for the vector database.\n",
        "\n",
        "### Integration Architecture:\n",
        "\n",
        "#### 1. **End-to-End Data Pipeline**\n",
        "\n",
        "```python\n",
        "# Complete integration pipeline\n",
        "class RAGScrapingIntegration:\n",
        "    \"\"\"Integrate web scraping with RAG system\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Initialize components\n",
        "        self.scraper = SelfHealingScraper()\n",
        "        self.preprocessor = RAGPreprocessor()\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=100,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
        "        )\n",
        "        self.embeddings = OpenAIEmbeddings(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            dimensions=1024\n",
        "        )\n",
        "        self.vector_store = PineconeVectorStore(\n",
        "            embedding=self.embeddings,\n",
        "            index=self.pinecone_index\n",
        "        )\n",
        "    \n",
        "    def scrape_and_ingest(self, urls):\n",
        "        \"\"\"Scrape URLs and ingest into RAG system\"\"\"\n",
        "        \n",
        "        all_documents = []\n",
        "        \n",
        "        for url in urls:\n",
        "            try:\n",
        "                # Scrape content\n",
        "                response = self.scraper.scrape_with_self_healing(url)\n",
        "                if not response:\n",
        "                    continue\n",
        "                \n",
        "                # Extract content\n",
        "                content = self.extract_content(response)\n",
        "                if not content:\n",
        "                    continue\n",
        "                \n",
        "                # Create document\n",
        "                doc = Document(\n",
        "                    page_content=content,\n",
        "                    metadata={\n",
        "                        'source': url,\n",
        "                        'scraped_at': datetime.now().isoformat(),\n",
        "                        'content_type': 'web_scraped'\n",
        "                    }\n",
        "                )\n",
        "                \n",
        "                all_documents.append(doc)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Process documents\n",
        "        processed_docs = self.preprocessor.preprocess_documents(all_documents)\n",
        "        \n",
        "        # Split into chunks\n",
        "        split_docs = self.text_splitter.split_documents(processed_docs)\n",
        "        \n",
        "        # Add to vector store\n",
        "        self.vector_store.add_documents(split_docs)\n",
        "        \n",
        "        return len(split_docs)\n",
        "```\n",
        "\n",
        "#### 2. **Content Extraction and Processing**\n",
        "\n",
        "```python\n",
        "class ContentExtractor:\n",
        "    \"\"\"Extract and process content from scraped responses\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.html_extractor = HTMLContentExtractor()\n",
        "        self.greek_cleaner = GreekTextCleaner()\n",
        "        self.quality_assessor = ContentQualityAssessor()\n",
        "    \n",
        "    def extract_content(self, response):\n",
        "        \"\"\"Extract clean content from HTTP response\"\"\"\n",
        "        \n",
        "        # Get HTML content\n",
        "        html_content = response.content.decode('utf-8', errors='ignore')\n",
        "        \n",
        "        # Extract clean text\n",
        "        clean_text = self.html_extractor.extract_clean_text(html_content)\n",
        "        \n",
        "        # Clean Greek text\n",
        "        clean_text = self.greek_cleaner.clean_greek_text(clean_text)\n",
        "        \n",
        "        # Assess quality\n",
        "        quality_score, quality_level, checks = self.quality_assessor.assess_quality(clean_text)\n",
        "        \n",
        "        # Skip low-quality content\n",
        "        if quality_score < 0.6:\n",
        "            print(f\"Skipping low-quality content: {quality_level}\")\n",
        "            return None\n",
        "        \n",
        "        return clean_text\n",
        "```\n",
        "\n",
        "### Performance Optimization:\n",
        "\n",
        "#### 1. **Batch Processing**\n",
        "\n",
        "```python\n",
        "class BatchProcessor:\n",
        "    \"\"\"Process multiple URLs in batches for efficiency\"\"\"\n",
        "    \n",
        "    def __init__(self, batch_size=10, max_workers=5):\n",
        "        self.batch_size = batch_size\n",
        "        self.max_workers = max_workers\n",
        "        self.scraper = SelfHealingScraper()\n",
        "        self.preprocessor = RAGPreprocessor()\n",
        "    \n",
        "    def process_urls_in_batches(self, urls):\n",
        "        \"\"\"Process URLs in batches with parallel processing\"\"\"\n",
        "        \n",
        "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "        \n",
        "        all_documents = []\n",
        "        \n",
        "        # Split URLs into batches\n",
        "        url_batches = [urls[i:i + self.batch_size] for i in range(0, len(urls), self.batch_size)]\n",
        "        \n",
        "        for batch in url_batches:\n",
        "            print(f\"Processing batch of {len(batch)} URLs\")\n",
        "            \n",
        "            # Process batch in parallel\n",
        "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "                future_to_url = {\n",
        "                    executor.submit(self.scrape_single_url, url): url \n",
        "                    for url in batch\n",
        "                }\n",
        "                \n",
        "                for future in as_completed(future_to_url):\n",
        "                    url = future_to_url[future]\n",
        "                    try:\n",
        "                        doc = future.result()\n",
        "                        if doc:\n",
        "                            all_documents.append(doc)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {url}: {e}\")\n",
        "            \n",
        "            # Add delay between batches\n",
        "            time.sleep(2)\n",
        "        \n",
        "        return all_documents\n",
        "    \n",
        "    def scrape_single_url(self, url):\n",
        "        \"\"\"Scrape a single URL and return document\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = self.scraper.scrape_with_self_healing(url)\n",
        "            if not response:\n",
        "                return None\n",
        "            \n",
        "            # Extract content\n",
        "            content = self.extract_content(response)\n",
        "            if not content:\n",
        "                return None\n",
        "            \n",
        "            # Create document\n",
        "            doc = Document(\n",
        "                page_content=content,\n",
        "                metadata={\n",
        "                    'source': url,\n",
        "                    'scraped_at': datetime.now().isoformat(),\n",
        "                    'content_type': 'web_scraped'\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            return doc\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            return None\n",
        "```\n",
        "\n",
        "#### 2. **Caching and Incremental Updates**\n",
        "\n",
        "```python\n",
        "class IncrementalUpdater:\n",
        "    \"\"\"Implement incremental updates for scraped content\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cache = {}  # In production, use Redis or similar\n",
        "        self.last_update = {}\n",
        "        self.update_interval = 3600  # 1 hour\n",
        "    \n",
        "    def should_update_url(self, url):\n",
        "        \"\"\"Check if URL should be updated\"\"\"\n",
        "        \n",
        "        now = time.time()\n",
        "        last_update = self.last_update.get(url, 0)\n",
        "        \n",
        "        return (now - last_update) > self.update_interval\n",
        "    \n",
        "    def get_cached_content(self, url):\n",
        "        \"\"\"Get cached content if available\"\"\"\n",
        "        \n",
        "        if url in self.cache:\n",
        "            return self.cache[url]\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    def cache_content(self, url, content, metadata):\n",
        "        \"\"\"Cache content for future use\"\"\"\n",
        "        \n",
        "        self.cache[url] = {\n",
        "            'content': content,\n",
        "            'metadata': metadata,\n",
        "            'cached_at': time.time()\n",
        "        }\n",
        "        \n",
        "        self.last_update[url] = time.time()\n",
        "    \n",
        "    def update_incremental(self, urls):\n",
        "        \"\"\"Update only URLs that need updating\"\"\"\n",
        "        \n",
        "        urls_to_update = []\n",
        "        \n",
        "        for url in urls:\n",
        "            if self.should_update_url(url):\n",
        "                urls_to_update.append(url)\n",
        "            else:\n",
        "                # Use cached content\n",
        "                cached = self.get_cached_content(url)\n",
        "                if cached:\n",
        "                    print(f\"Using cached content for {url}\")\n",
        "        \n",
        "        if urls_to_update:\n",
        "            print(f\"Updating {len(urls_to_update)} URLs\")\n",
        "            # Process URLs that need updating\n",
        "            return self.process_urls_in_batches(urls_to_update)\n",
        "        \n",
        "        return []\n",
        "```\n",
        "\n",
        "### Quality Assurance:\n",
        "\n",
        "#### 1. **Content Validation**\n",
        "\n",
        "```python\n",
        "class ContentValidator:\n",
        "    \"\"\"Validate content quality before adding to RAG system\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.min_length = 100\n",
        "        self.max_length = 5000\n",
        "        self.min_quality_score = 0.6\n",
        "        self.required_entities = ['teams', 'competitions']\n",
        "    \n",
        "    def validate_document(self, doc):\n",
        "        \"\"\"Validate document quality and relevance\"\"\"\n",
        "        \n",
        "        content = doc.page_content\n",
        "        metadata = doc.metadata\n",
        "        \n",
        "        # Check length\n",
        "        if len(content) < self.min_length or len(content) > self.max_length:\n",
        "            return False, \"Invalid content length\"\n",
        "        \n",
        "        # Check quality score\n",
        "        quality_score = metadata.get('quality_score', 0)\n",
        "        if quality_score < self.min_quality_score:\n",
        "            return False, \"Low quality score\"\n",
        "        \n",
        "        # Check for required entities\n",
        "        entities = metadata.get('entities', {})\n",
        "        if not any(entities.get(entity, []) for entity in self.required_entities):\n",
        "            return False, \"Missing required entities\"\n",
        "        \n",
        "        # Check language\n",
        "        language = metadata.get('language', 'unknown')\n",
        "        if language not in ['greek', 'mixed']:\n",
        "            return False, \"Not Greek content\"\n",
        "        \n",
        "        return True, \"Valid document\"\n",
        "    \n",
        "    def validate_batch(self, documents):\n",
        "        \"\"\"Validate a batch of documents\"\"\"\n",
        "        \n",
        "        valid_docs = []\n",
        "        invalid_docs = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            is_valid, reason = self.validate_document(doc)\n",
        "            \n",
        "            if is_valid:\n",
        "                valid_docs.append(doc)\n",
        "            else:\n",
        "                invalid_docs.append((doc, reason))\n",
        "                print(f\"Invalid document: {reason}\")\n",
        "        \n",
        "        return valid_docs, invalid_docs\n",
        "```\n",
        "\n",
        "#### 2. **Duplicate Detection and Removal**\n",
        "\n",
        "```python\n",
        "class DuplicateDetector:\n",
        "    \"\"\"Detect and remove duplicate content\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.content_hashes = set()\n",
        "        self.similarity_threshold = 0.8\n",
        "    \n",
        "    def is_duplicate(self, content):\n",
        "        \"\"\"Check if content is duplicate\"\"\"\n",
        "        \n",
        "        # Create content hash\n",
        "        content_hash = self.create_content_hash(content)\n",
        "        \n",
        "        if content_hash in self.content_hashes:\n",
        "            return True\n",
        "        \n",
        "        # Add to hashes\n",
        "        self.content_hashes.add(content_hash)\n",
        "        return False\n",
        "    \n",
        "    def create_content_hash(self, content):\n",
        "        \"\"\"Create hash for content deduplication\"\"\"\n",
        "        \n",
        "        # Normalize content\n",
        "        normalized = re.sub(r'\\s+', ' ', content.strip().lower())\n",
        "        \n",
        "        # Create hash\n",
        "        return hashlib.md5(normalized.encode()).hexdigest()\n",
        "    \n",
        "    def remove_duplicates(self, documents):\n",
        "        \"\"\"Remove duplicate documents\"\"\"\n",
        "        \n",
        "        unique_docs = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            if not self.is_duplicate(doc.page_content):\n",
        "                unique_docs.append(doc)\n",
        "            else:\n",
        "                print(f\"Duplicate document found: {doc.metadata.get('source', 'Unknown')}\")\n",
        "        \n",
        "        return unique_docs\n",
        "```\n",
        "\n",
        "### Monitoring and Metrics:\n",
        "\n",
        "#### 1. **Performance Metrics**\n",
        "\n",
        "```python\n",
        "class ScrapingMetrics:\n",
        "    \"\"\"Track scraping and RAG integration metrics\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            'total_urls_scraped': 0,\n",
        "            'successful_scrapes': 0,\n",
        "            'failed_scrapes': 0,\n",
        "            'documents_processed': 0,\n",
        "            'documents_added_to_vector_db': 0,\n",
        "            'duplicate_documents_removed': 0,\n",
        "            'low_quality_documents_skipped': 0,\n",
        "            'processing_time': 0,\n",
        "            'vector_db_size': 0,\n",
        "        }\n",
        "    \n",
        "    def record_scrape_attempt(self, url, success):\n",
        "        \"\"\"Record scrape attempt\"\"\"\n",
        "        \n",
        "        self.metrics['total_urls_scraped'] += 1\n",
        "        \n",
        "        if success:\n",
        "            self.metrics['successful_scrapes'] += 1\n",
        "        else:\n",
        "            self.metrics['failed_scrapes'] += 1\n",
        "    \n",
        "    def record_document_processing(self, total_docs, added_docs, duplicates, skipped):\n",
        "        \"\"\"Record document processing results\"\"\"\n",
        "        \n",
        "        self.metrics['documents_processed'] += total_docs\n",
        "        self.metrics['documents_added_to_vector_db'] += added_docs\n",
        "        self.metrics['duplicate_documents_removed'] += duplicates\n",
        "        self.metrics['low_quality_documents_skipped'] += skipped\n",
        "    \n",
        "    def record_processing_time(self, start_time, end_time):\n",
        "        \"\"\"Record processing time\"\"\"\n",
        "        \n",
        "        processing_time = end_time - start_time\n",
        "        self.metrics['processing_time'] = processing_time\n",
        "    \n",
        "    def get_metrics(self):\n",
        "        \"\"\"Get current metrics\"\"\"\n",
        "        return self.metrics.copy()\n",
        "    \n",
        "    def get_success_rate(self):\n",
        "        \"\"\"Get scraping success rate\"\"\"\n",
        "        \n",
        "        total = self.metrics['total_urls_scraped']\n",
        "        if total == 0:\n",
        "            return 0\n",
        "        \n",
        "        return self.metrics['successful_scrapes'] / total\n",
        "    \n",
        "    def get_processing_efficiency(self):\n",
        "        \"\"\"Get document processing efficiency\"\"\"\n",
        "        \n",
        "        total = self.metrics['documents_processed']\n",
        "        if total == 0:\n",
        "            return 0\n",
        "        \n",
        "        return self.metrics['documents_added_to_vector_db'] / total\n",
        "```\n",
        "\n",
        "#### 2. **Real-time Monitoring**\n",
        "\n",
        "```python\n",
        "class RealTimeMonitor:\n",
        "    \"\"\"Monitor scraping and RAG integration in real-time\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.metrics = ScrapingMetrics()\n",
        "        self.start_time = time.time()\n",
        "        self.last_update = time.time()\n",
        "    \n",
        "    def monitor_scraping_progress(self, urls, processed_urls):\n",
        "        \"\"\"Monitor scraping progress\"\"\"\n",
        "        \n",
        "        progress = len(processed_urls) / len(urls) * 100\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        \n",
        "        print(f\"Scraping Progress: {progress:.1f}% ({len(processed_urls)}/{len(urls)})\")\n",
        "        print(f\"Elapsed Time: {elapsed_time:.1f}s\")\n",
        "        \n",
        "        # Calculate ETA\n",
        "        if len(processed_urls) > 0:\n",
        "            avg_time_per_url = elapsed_time / len(processed_urls)\n",
        "            remaining_urls = len(urls) - len(processed_urls)\n",
        "            eta = remaining_urls * avg_time_per_url\n",
        "            print(f\"ETA: {eta:.1f}s\")\n",
        "    \n",
        "    def monitor_quality_metrics(self):\n",
        "        \"\"\"Monitor content quality metrics\"\"\"\n",
        "        \n",
        "        success_rate = self.metrics.get_success_rate()\n",
        "        efficiency = self.metrics.get_processing_efficiency()\n",
        "        \n",
        "        print(f\"Success Rate: {success_rate:.1%}\")\n",
        "        print(f\"Processing Efficiency: {efficiency:.1%}\")\n",
        "        \n",
        "        # Check for quality issues\n",
        "        if success_rate < 0.8:\n",
        "            print(\"WARNING: Low success rate detected\")\n",
        "        \n",
        "        if efficiency < 0.7:\n",
        "            print(\"WARNING: Low processing efficiency detected\")\n",
        "```\n",
        "\n",
        "### Complete Integration Example:\n",
        "\n",
        "```python\n",
        "class CompleteRAGScrapingIntegration:\n",
        "    \"\"\"Complete integration of web scraping with RAG system\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.scraper = SelfHealingScraper()\n",
        "        self.batch_processor = BatchProcessor()\n",
        "        self.incremental_updater = IncrementalUpdater()\n",
        "        self.validator = ContentValidator()\n",
        "        self.duplicate_detector = DuplicateDetector()\n",
        "        self.monitor = RealTimeMonitor()\n",
        "        self.vector_store = PineconeVectorStore(\n",
        "            embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
        "            index=pinecone_index\n",
        "        )\n",
        "    \n",
        "    def run_complete_pipeline(self, urls):\n",
        "        \"\"\"Run complete scraping and RAG integration pipeline\"\"\"\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(\"Starting complete RAG scraping pipeline...\")\n",
        "        \n",
        "        # Step 1: Scrape URLs\n",
        "        print(\"Step 1: Scraping URLs...\")\n",
        "        documents = self.batch_processor.process_urls_in_batches(urls)\n",
        "        \n",
        "        # Step 2: Validate documents\n",
        "        print(\"Step 2: Validating documents...\")\n",
        "        valid_docs, invalid_docs = self.validator.validate_batch(documents)\n",
        "        \n",
        "        # Step 3: Remove duplicates\n",
        "        print(\"Step 3: Removing duplicates...\")\n",
        "        unique_docs = self.duplicate_detector.remove_duplicates(valid_docs)\n",
        "        \n",
        "        # Step 4: Process for RAG\n",
        "        print(\"Step 4: Processing for RAG...\")\n",
        "        processed_docs = self.preprocessor.preprocess_documents(unique_docs)\n",
        "        \n",
        "        # Step 5: Add to vector database\n",
        "        print(\"Step 5: Adding to vector database...\")\n",
        "        self.vector_store.add_documents(processed_docs)\n",
        "        \n",
        "        # Step 6: Record metrics\n",
        "        end_time = time.time()\n",
        "        self.monitor.metrics.record_processing_time(start_time, end_time)\n",
        "        self.monitor.metrics.record_document_processing(\n",
        "            len(documents), len(processed_docs), \n",
        "            len(valid_docs) - len(unique_docs), len(invalid_docs)\n",
        "        )\n",
        "        \n",
        "        # Step 7: Report results\n",
        "        self.report_results()\n",
        "        \n",
        "        return len(processed_docs)\n",
        "    \n",
        "    def report_results(self):\n",
        "        \"\"\"Report pipeline results\"\"\"\n",
        "        \n",
        "        metrics = self.monitor.metrics.get_metrics()\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"PIPELINE RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Total URLs scraped: {metrics['total_urls_scraped']}\")\n",
        "        print(f\"Successful scrapes: {metrics['successful_scrapes']}\")\n",
        "        print(f\"Failed scrapes: {metrics['failed_scrapes']}\")\n",
        "        print(f\"Documents processed: {metrics['documents_processed']}\")\n",
        "        print(f\"Documents added to vector DB: {metrics['documents_added_to_vector_db']}\")\n",
        "        print(f\"Duplicates removed: {metrics['duplicate_documents_removed']}\")\n",
        "        print(f\"Low quality skipped: {metrics['low_quality_documents_skipped']}\")\n",
        "        print(f\"Processing time: {metrics['processing_time']:.1f}s\")\n",
        "        print(f\"Success rate: {self.monitor.metrics.get_success_rate():.1%}\")\n",
        "        print(f\"Processing efficiency: {self.monitor.metrics.get_processing_efficiency():.1%}\")\n",
        "        print(\"=\"*50)\n",
        "```\n",
        "\n",
        "### Benefits for Our RAG Chatbot:\n",
        "\n",
        "1. **Seamless Integration**: Smooth data flow from scraping to RAG system\n",
        "2. **Quality Assurance**: Multiple validation layers ensure high-quality data\n",
        "3. **Performance Optimization**: Batch processing and caching for efficiency\n",
        "4. **Real-time Monitoring**: Track progress and identify issues quickly\n",
        "5. **Incremental Updates**: Only update content that has changed\n",
        "6. **Duplicate Prevention**: Avoid storing duplicate content\n",
        "7. **Error Recovery**: Robust error handling throughout the pipeline\n",
        "8. **Scalability**: Handle large amounts of data efficiently\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🎯 Summary\n",
        "\n",
        "This lesson covered the essential Web Scraping concepts used in our Greek Derby RAG chatbot:\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Web Scraping Fundamentals**: Data extraction, HTML parsing, and content processing\n",
        "2. **HTML Parsing & CSS Selectors**: Advanced parsing techniques and selector strategies\n",
        "3. **Anti-Bot Detection**: User agent rotation, rate limiting, and ethical scraping practices\n",
        "4. **Data Cleaning & Preprocessing**: Content quality assessment and Greek language processing\n",
        "5. **Error Handling & Resilience**: Circuit breakers, fallback strategies, and self-healing systems\n",
        "6. **RAG Integration**: End-to-end pipeline from scraping to vector database\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- **Practice**: Try building your own web scraper with these techniques\n",
        "- **Explore**: Learn about advanced scraping tools like Scrapy and Playwright\n",
        "- **Advanced**: Study machine learning approaches for content extraction\n",
        "- **Production**: Implement monitoring and alerting for scraping systems\n",
        "\n",
        "### Project Structure:\n",
        "\n",
        "```\n",
        "rag-langchain-langgraph/\n",
        "├── backend/\n",
        "│   ├── scheduler/\n",
        "│   │   ├── update_vector_db.py      # Web scraping scheduler\n",
        "│   │   └── config.py                # Scraping configuration\n",
        "│   └── standalone-service/\n",
        "│       └── greek_derby_chatbot.py   # Main scraping logic\n",
        "├── educational-content/\n",
        "│   └── 05_web_scraping_concepts.ipynb  # This lesson\n",
        "└── .env                              # Environment variables\n",
        "```\n",
        "\n",
        "### Web Scraping Benefits for Our RAG Chatbot:\n",
        "\n",
        "1. **Real-time Data**: Always current information from Gazzetta.gr\n",
        "2. **Comprehensive Coverage**: Access to vast amounts of Greek football content\n",
        "3. **Automated Updates**: No manual maintenance required\n",
        "4. **Quality Assurance**: Multiple validation layers ensure high-quality data\n",
        "5. **Error Resilience**: Robust error handling and recovery mechanisms\n",
        "6. **Performance**: Optimized processing and caching for efficiency\n",
        "7. **Scalability**: Can handle multiple sources and large amounts of data\n",
        "8. **Integration**: Seamless connection with RAG system for optimal performance\n",
        "\n",
        "This architecture provides a solid foundation for building robust, scalable, and production-ready web scraping systems! 🚀\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

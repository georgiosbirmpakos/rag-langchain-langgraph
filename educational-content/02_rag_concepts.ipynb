{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† RAG (Retrieval-Augmented Generation) Concepts - Greek Derby Chatbot\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this lesson, you will understand:\n",
        "- What RAG is and why it's revolutionary for chatbots\n",
        "- How vector databases work for semantic search\n",
        "- LangChain framework and its components\n",
        "- LangGraph for complex AI workflows\n",
        "- Embeddings and their role in similarity search\n",
        "- Memory management in conversational AI\n",
        "- Web scraping and data processing for knowledge bases\n",
        "\n",
        "---\n",
        "\n",
        "## Q1: What is RAG and why is it better than traditional chatbots?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is a powerful AI technique that combines **retrieval** of relevant information with **generation** of responses. It's like giving an AI assistant access to a vast library of information before answering questions.\n",
        "\n",
        "### Traditional Chatbots vs RAG:\n",
        "\n",
        "**Traditional Chatbots:**\n",
        "- Rely only on pre-trained knowledge (limited to training data)\n",
        "- Can't access real-time or specific information\n",
        "- Often give generic or outdated answers\n",
        "- Limited to what they \"learned\" during training\n",
        "\n",
        "**RAG Chatbots:**\n",
        "- Can access external knowledge sources (websites, documents, databases)\n",
        "- Provide up-to-date, specific information\n",
        "- Give contextually relevant answers\n",
        "- Can be updated with new information without retraining\n",
        "\n",
        "### How RAG Works:\n",
        "\n",
        "1. **Retrieval**: Find relevant information from a knowledge base\n",
        "2. **Augmentation**: Combine retrieved information with the user's question\n",
        "3. **Generation**: Use an LLM to generate a response based on both\n",
        "\n",
        "### Our Greek Derby RAG System:\n",
        "\n",
        "```python\n",
        "# Simplified RAG flow\n",
        "def chat(self, question: str) -> str:\n",
        "    # 1. RETRIEVAL: Find relevant documents\n",
        "    relevant_docs = self.vector_store.similarity_search(question, k=4)\n",
        "    \n",
        "    # 2. AUGMENTATION: Combine question with context\n",
        "    context = self._format_context(relevant_docs)\n",
        "    augmented_prompt = f\"Context: {context}\\nQuestion: {question}\"\n",
        "    \n",
        "    # 3. GENERATION: Generate response using LLM\n",
        "    response = self.llm.invoke(augmented_prompt)\n",
        "    return response\n",
        "```\n",
        "\n",
        "### Why RAG is Perfect for Our Greek Derby Chatbot:\n",
        "- **Real-time Information**: Can access latest news from Gazzetta.gr\n",
        "- **Specific Knowledge**: Knows about Olympiakos vs Panathinaikos specifically\n",
        "- **Accurate Facts**: Retrieves actual information rather than generating from memory\n",
        "- **Updatable**: Can add new information without retraining the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2: What are embeddings and how do they enable semantic search?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Embeddings are numerical representations of text that capture semantic meaning. They convert words, sentences, or documents into high-dimensional vectors that can be compared mathematically.\n",
        "\n",
        "### What are Embeddings?\n",
        "\n",
        "Think of embeddings as a \"translation\" of text into numbers that computers can understand and compare. Similar texts get similar numbers, while different texts get different numbers.\n",
        "\n",
        "### How Embeddings Work:\n",
        "\n",
        "1. **Text Input**: \"Olympiakos vs Panathinaikos derby\"\n",
        "2. **Embedding Model**: Converts to vector like `[0.1, -0.3, 0.8, ...]`\n",
        "3. **Mathematical Comparison**: Can calculate similarity between vectors\n",
        "\n",
        "### Our Embedding Setup:\n",
        "\n",
        "```python\n",
        "def _init_embeddings(self):\n",
        "    \"\"\"Initialize embeddings model\"\"\"\n",
        "    self.embeddings = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        dimensions=1024\n",
        "    )\n",
        "```\n",
        "\n",
        "### Key Parameters:\n",
        "- **Model**: `text-embedding-3-small` - OpenAI's efficient embedding model\n",
        "- **Dimensions**: `1024` - Each text becomes a 1024-dimensional vector\n",
        "- **Cost**: Small model = lower cost, good performance\n",
        "\n",
        "### Semantic Search Example:\n",
        "\n",
        "```python\n",
        "# These would have similar embeddings:\n",
        "\"Olympiakos football team\"\n",
        "\"ŒüŒªœÖŒºœÄŒπŒ±Œ∫œåœÇ œÄŒøŒ¥ŒøœÉœÜŒ±ŒπœÅŒπŒ∫ŒÆ ŒøŒºŒ¨Œ¥Œ±\"\n",
        "\"Red and white team from Piraeus\"\n",
        "\n",
        "# These would have different embeddings:\n",
        "\"Olympiakos football team\"\n",
        "\"Panathinaikos basketball team\"\n",
        "\"Cooking recipes\"\n",
        "```\n",
        "\n",
        "### Why 1024 Dimensions?\n",
        "\n",
        "- **More Dimensions**: Better semantic understanding\n",
        "- **Computational Cost**: Balance between accuracy and speed\n",
        "- **Storage**: Each document needs 1024 numbers stored\n",
        "- **Similarity**: More precise similarity calculations\n",
        "\n",
        "### Embedding Process in Our System:\n",
        "\n",
        "1. **Document Chunking**: Split large documents into smaller pieces\n",
        "2. **Embedding Generation**: Convert each chunk to a 1024-dimensional vector\n",
        "3. **Vector Storage**: Store in Pinecone vector database\n",
        "4. **Query Embedding**: Convert user question to vector\n",
        "5. **Similarity Search**: Find most similar document vectors\n",
        "\n",
        "### Text Chunking Strategy - Why Size Matters:\n",
        "\n",
        "Our system uses `RecursiveCharacterTextSplitter` with specific parameters:\n",
        "\n",
        "```python\n",
        "self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,        # Characters per chunk\n",
        "    chunk_overlap=100,     # Overlap between chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
        ")\n",
        "```\n",
        "\n",
        "### Chunk Size Guidelines:\n",
        "\n",
        "**500 Characters (Our Choice):**\n",
        "- **Pros**: \n",
        "  - Fits in most LLM context windows\n",
        "  - Contains enough context for meaningful answers\n",
        "  - Good balance between specificity and context\n",
        "  - Lower embedding costs\n",
        "- **Cons**: \n",
        "  - May split related information across chunks\n",
        "  - Might miss broader context\n",
        "\n",
        "**1000 Characters:**\n",
        "- **Pros**: \n",
        "  - More context per chunk\n",
        "  - Better for complex questions\n",
        "  - Reduces number of chunks needed\n",
        "- **Cons**: \n",
        "  - Higher embedding costs\n",
        "  - May include irrelevant information\n",
        "  - Harder to find specific details\n",
        "\n",
        "**250 Characters:**\n",
        "- **Pros**: \n",
        "  - Very specific and focused\n",
        "  - Lower costs\n",
        "  - Fast retrieval\n",
        "- **Cons**: \n",
        "  - May lack sufficient context\n",
        "  - More chunks to process\n",
        "  - Risk of losing meaning\n",
        "\n",
        "### Basic Rules for Chunk Size:\n",
        "\n",
        "1. **Content Type**:\n",
        "   - **News Articles**: 500-800 characters\n",
        "   - **Technical Docs**: 300-500 characters\n",
        "   - **Conversational Text**: 400-600 characters\n",
        "   - **Code**: 200-400 characters\n",
        "\n",
        "2. **LLM Context Window**:\n",
        "   - **GPT-4**: Can handle larger chunks (1000+)\n",
        "   - **GPT-3.5**: Better with smaller chunks (500-800)\n",
        "   - **Local Models**: Often need smaller chunks (300-500)\n",
        "\n",
        "3. **Question Complexity**:\n",
        "   - **Simple Facts**: Smaller chunks (300-500)\n",
        "   - **Complex Analysis**: Larger chunks (600-1000)\n",
        "   - **Comparative Questions**: Medium chunks (500-700)\n",
        "\n",
        "4. **Cost Considerations**:\n",
        "   - **More Chunks** = Higher embedding costs\n",
        "   - **Larger Chunks** = Higher LLM processing costs\n",
        "   - **Balance**: Find the sweet spot for your use case\n",
        "\n",
        "### Our Greek Derby Chatbot Reasoning:\n",
        "\n",
        "We chose **500 characters** because:\n",
        "- **Greek Text**: Greek characters are more information-dense\n",
        "- **Sports Content**: News articles have natural paragraph breaks\n",
        "- **Question Types**: Most questions need 1-2 sentences of context\n",
        "- **Cost Efficiency**: Balance between quality and cost\n",
        "- **Retrieval Accuracy**: Small enough to be specific, large enough for context\n",
        "\n",
        "### Chunk Overlap Strategy:\n",
        "\n",
        "**100 Character Overlap** ensures:\n",
        "- **Context Preservation**: Important information isn't lost at boundaries\n",
        "- **Better Retrieval**: Related information spans multiple chunks\n",
        "- **Smoother Processing**: No gaps in information flow\n",
        "\n",
        "### Testing Your Chunk Size:\n",
        "\n",
        "```python\n",
        "# Test different chunk sizes\n",
        "def test_chunk_sizes(text, sizes=[250, 500, 750, 1000]):\n",
        "    for size in sizes:\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=size,\n",
        "            chunk_overlap=size//5  # 20% overlap\n",
        "        )\n",
        "        chunks = splitter.split_text(text)\n",
        "        print(f\"Size {size}: {len(chunks)} chunks\")\n",
        "        print(f\"Average chunk length: {sum(len(c) for c in chunks)/len(chunks):.0f}\")\n",
        "        print(\"---\")\n",
        "```\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "1. **Start with 500**: Good default for most use cases\n",
        "2. **Test with Your Data**: Try different sizes with your specific content\n",
        "3. **Monitor Performance**: Track retrieval quality and costs\n",
        "4. **Consider Language**: Some languages need different chunk sizes\n",
        "5. **Iterate**: Adjust based on user feedback and performance metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q3: How do vector databases work and why do we use Pinecone?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Vector databases are specialized databases designed to store and search high-dimensional vectors efficiently. They're perfect for RAG systems because they can quickly find similar vectors (documents) based on semantic similarity.\n",
        "\n",
        "### What is a Vector Database?\n",
        "\n",
        "A vector database stores:\n",
        "- **Vectors**: High-dimensional arrays (our 1024-dimensional embeddings)\n",
        "- **Metadata**: Additional information about each vector\n",
        "- **Indexes**: Optimized structures for fast similarity search\n",
        "\n",
        "### Why Not Regular Databases?\n",
        "\n",
        "**Regular Database (SQL/NoSQL):**\n",
        "- Stores text as strings\n",
        "- Uses exact matches or simple text search\n",
        "- Can't understand semantic meaning\n",
        "- Slow for similarity searches\n",
        "\n",
        "**Vector Database:**\n",
        "- Stores text as numerical vectors\n",
        "- Understands semantic similarity\n",
        "- Optimized for similarity search\n",
        "- Fast even with millions of vectors\n",
        "\n",
        "### Our Pinecone Setup:\n",
        "\n",
        "```python\n",
        "def _init_vector_store(self):\n",
        "    \"\"\"Initialize vector store\"\"\"\n",
        "    pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
        "    self.index = pc.Index(os.getenv('PINECONE_GREEK_DERBY_INDEX_NAME'))\n",
        "    self.vector_store = PineconeVectorStore(embedding=self.embeddings, index=self.index)\n",
        "```\n",
        "\n",
        "### Pinecone Configuration:\n",
        "\n",
        "```python\n",
        "# Index configuration\n",
        "{\n",
        "    \"name\": \"greek-derby-index\",\n",
        "    \"dimension\": 1024,        # Must match embedding dimensions\n",
        "    \"metric\": \"cosine\",       # Similarity metric\n",
        "    \"pods\": 1,               # Number of pods (scaling)\n",
        "    \"replicas\": 1            # Number of replicas (reliability)\n",
        "}\n",
        "```\n",
        "\n",
        "### Why Pinecone?\n",
        "\n",
        "1. **Performance**: Optimized for vector similarity search\n",
        "2. **Scalability**: Can handle millions of vectors\n",
        "3. **Managed Service**: No infrastructure management needed\n",
        "4. **LangChain Integration**: Easy to use with our framework\n",
        "5. **Real-time**: Fast query responses\n",
        "6. **Reliability**: Built-in redundancy and backup\n",
        "\n",
        "### How Vector Search Works:\n",
        "\n",
        "```python\n",
        "# 1. User asks: \"What is the history of the derby?\"\n",
        "query = \"What is the history of the derby?\"\n",
        "\n",
        "# 2. Convert question to embedding\n",
        "query_vector = embeddings.embed_query(query)\n",
        "# Result: [0.1, -0.3, 0.8, 0.2, ...] (1024 dimensions)\n",
        "\n",
        "# 3. Search for similar vectors\n",
        "similar_docs = vector_store.similarity_search(query, k=4)\n",
        "# Returns 4 most similar document chunks\n",
        "\n",
        "# 4. Use retrieved documents as context\n",
        "context = format_documents(similar_docs)\n",
        "```\n",
        "\n",
        "### Similarity Metrics:\n",
        "\n",
        "**Cosine Similarity** (what we use):\n",
        "- Measures angle between vectors\n",
        "- Range: -1 to 1 (1 = identical, 0 = orthogonal, -1 = opposite)\n",
        "- Good for text similarity\n",
        "- Scale-invariant (ignores vector magnitude)\n",
        "\n",
        "**Other Metrics:**\n",
        "- **Euclidean Distance**: Straight-line distance\n",
        "- **Dot Product**: Vector multiplication\n",
        "- **Manhattan Distance**: Sum of absolute differences\n",
        "\n",
        "### Storage and Retrieval Process:\n",
        "\n",
        "1. **Indexing**: Store document chunks as vectors\n",
        "2. **Querying**: Convert question to vector\n",
        "3. **Searching**: Find most similar vectors\n",
        "4. **Ranking**: Sort by similarity score\n",
        "5. **Retrieval**: Return top-k most relevant chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q4: What is LangChain and how does it help us build RAG systems?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "LangChain is a powerful framework for building applications with Large Language Models (LLMs). It provides a unified interface for working with different LLM providers and makes it easy to build complex AI applications like our RAG chatbot.\n",
        "\n",
        "### What is LangChain?\n",
        "\n",
        "LangChain is like a \"Swiss Army knife\" for AI applications. It provides:\n",
        "- **Abstractions**: Common patterns for AI applications\n",
        "- **Components**: Reusable building blocks\n",
        "- **Chains**: Ways to combine components\n",
        "- **Memory**: Conversation state management\n",
        "- **Tools**: Integration with external systems\n",
        "\n",
        "### Key LangChain Concepts:\n",
        "\n",
        "1. **LLMs**: Language models (GPT, Claude, etc.)\n",
        "2. **Prompts**: Templates for structuring inputs\n",
        "3. **Chains**: Sequences of operations\n",
        "4. **Memory**: Storing conversation history\n",
        "5. **Agents**: AI that can use tools\n",
        "6. **Vector Stores**: Databases for embeddings\n",
        "\n",
        "### Our LangChain Usage:\n",
        "\n",
        "```python\n",
        "# 1. Language Model\n",
        "from langchain.chat_models import init_chat_model\n",
        "self.llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
        "\n",
        "# 2. Embeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# 3. Vector Store\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "self.vector_store = PineconeVectorStore(embedding=self.embeddings, index=self.index)\n",
        "\n",
        "# 4. Memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "self.memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# 5. Text Splitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "```\n",
        "\n",
        "### Why Use LangChain?\n",
        "\n",
        "**Without LangChain:**\n",
        "```python\n",
        "# Manual implementation - lots of boilerplate\n",
        "import openai\n",
        "import pinecone\n",
        "\n",
        "# Complex setup for each component\n",
        "openai.api_key = \"your-key\"\n",
        "pinecone.init(api_key=\"your-key\", environment=\"your-env\")\n",
        "\n",
        "# Manual prompt construction\n",
        "prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "```\n",
        "\n",
        "**With LangChain:**\n",
        "```python\n",
        "# Clean, simple, and reusable\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=self.llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=self.vector_store.as_retriever()\n",
        ")\n",
        "response = qa_chain.run(question)\n",
        "```\n",
        "\n",
        "### LangChain Components We Use:\n",
        "\n",
        "1. **LLM Wrapper**: `init_chat_model()` - Unified interface for different LLMs\n",
        "2. **Embeddings**: `OpenAIEmbeddings` - Convert text to vectors\n",
        "3. **Vector Store**: `PineconeVectorStore` - Store and search vectors\n",
        "4. **Memory**: `ConversationBufferMemory` - Remember conversation history\n",
        "5. **Text Splitter**: `RecursiveCharacterTextSplitter` - Split documents into chunks\n",
        "6. **Prompts**: `ChatPromptTemplate` - Structure prompts consistently\n",
        "\n",
        "### LangChain Benefits:\n",
        "\n",
        "- **Abstraction**: Hide complexity of different LLM providers\n",
        "- **Composability**: Mix and match components easily\n",
        "- **Standardization**: Common patterns across AI applications\n",
        "- **Memory Management**: Built-in conversation memory\n",
        "- **Error Handling**: Robust error handling and retries\n",
        "- **Extensibility**: Easy to add custom components\n",
        "\n",
        "### LangChain Chains:\n",
        "\n",
        "Chains combine multiple components into workflows:\n",
        "\n",
        "```python\n",
        "# RetrievalQA Chain (what we could use)\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=self.llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=self.vector_store.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# This automatically:\n",
        "# 1. Retrieves relevant documents\n",
        "# 2. Formats them as context\n",
        "# 3. Sends to LLM with question\n",
        "# 4. Returns the answer\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q5: What is LangGraph and how do we use it for complex AI workflows?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "LangGraph is a library for building stateful, multi-actor applications with LLMs. It's like a \"workflow engine\" for AI applications, allowing you to create complex, branching logic that can handle different scenarios and maintain state across multiple steps.\n",
        "\n",
        "### What is LangGraph?\n",
        "\n",
        "LangGraph extends LangChain by adding:\n",
        "- **State Management**: Persistent state across workflow steps\n",
        "- **Conditional Logic**: Different paths based on conditions\n",
        "- **Human-in-the-Loop**: Pause for human input when needed\n",
        "- **Cycles and Loops**: Repeat steps until conditions are met\n",
        "- **Multi-Agent Systems**: Multiple AI agents working together\n",
        "\n",
        "### Our LangGraph Implementation:\n",
        "\n",
        "```python\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GreekDerbyState(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "def _init_rag_system(self):\n",
        "    \"\"\"Initialize RAG system with LangGraph\"\"\"\n",
        "    # Create the workflow graph\n",
        "    workflow = StateGraph(GreekDerbyState)\n",
        "    \n",
        "    # Add nodes (workflow steps)\n",
        "    workflow.add_node(\"retrieve\", self._retrieve_documents)\n",
        "    workflow.add_node(\"generate\", self._generate_answer)\n",
        "    \n",
        "    # Define the flow\n",
        "    workflow.add_edge(START, \"retrieve\")\n",
        "    workflow.add_edge(\"retrieve\", \"generate\")\n",
        "    workflow.add_edge(\"generate\", END)\n",
        "    \n",
        "    # Compile the graph\n",
        "    self.rag_chain = workflow.compile()\n",
        "```\n",
        "\n",
        "### Why Use LangGraph Instead of Simple Chains?\n",
        "\n",
        "**Simple Chain Approach:**\n",
        "```python\n",
        "# Linear, no branching\n",
        "def simple_rag(question):\n",
        "    docs = retrieve(question)\n",
        "    answer = generate(question, docs)\n",
        "    return answer\n",
        "```\n",
        "\n",
        "**LangGraph Approach:**\n",
        "```python\n",
        "# Complex workflow with conditions\n",
        "def complex_rag(question):\n",
        "    docs = retrieve(question)\n",
        "    \n",
        "    if len(docs) < 2:  # Not enough context\n",
        "        docs = search_more_sources(question)\n",
        "    \n",
        "    if is_ambiguous(question):  # Need clarification\n",
        "        return ask_for_clarification(question)\n",
        "    \n",
        "    answer = generate(question, docs)\n",
        "    \n",
        "    if needs_follow_up(answer):  # Generate follow-up questions\n",
        "        follow_ups = generate_follow_ups(answer)\n",
        "        return answer, follow_ups\n",
        "    \n",
        "    return answer\n",
        "```\n",
        "\n",
        "### Our Workflow Steps:\n",
        "\n",
        "1. **Retrieve Documents** (`retrieve` node):\n",
        "   ```python\n",
        "   def _retrieve_documents(self, state: GreekDerbyState):\n",
        "       \"\"\"Retrieve relevant documents for the question\"\"\"\n",
        "       question = state[\"question\"]\n",
        "       docs = self.vector_store.similarity_search(question, k=4)\n",
        "       return {\"context\": docs}\n",
        "   ```\n",
        "\n",
        "2. **Generate Answer** (`generate` node):\n",
        "   ```python\n",
        "   def _generate_answer(self, state: GreekDerbyState):\n",
        "       \"\"\"Generate answer using retrieved context\"\"\"\n",
        "       question = state[\"question\"]\n",
        "       context = state[\"context\"]\n",
        "       \n",
        "       # Format context and create prompt\n",
        "       formatted_context = self._format_context(context)\n",
        "       prompt = self._create_prompt(question, formatted_context)\n",
        "       \n",
        "       # Generate response\n",
        "       response = self.llm.invoke(prompt)\n",
        "       return {\"answer\": response}\n",
        "   ```\n",
        "\n",
        "### State Management:\n",
        "\n",
        "The `GreekDerbyState` TypedDict defines what data flows through our workflow:\n",
        "\n",
        "```python\n",
        "class GreekDerbyState(TypedDict):\n",
        "    question: str        # User's question\n",
        "    context: List[Document]  # Retrieved documents\n",
        "    answer: str         # Generated answer\n",
        "```\n",
        "\n",
        "Each node can:\n",
        "- **Read** from the state\n",
        "- **Update** the state\n",
        "- **Pass** data to the next node\n",
        "\n",
        "### Advanced LangGraph Features We Could Use:\n",
        "\n",
        "1. **Conditional Edges**:\n",
        "   ```python\n",
        "   def should_retrieve_more(state):\n",
        "       return len(state[\"context\"]) < 2\n",
        "   \n",
        "   workflow.add_conditional_edges(\n",
        "       \"retrieve\",\n",
        "       should_retrieve_more,\n",
        "       {\n",
        "           True: \"retrieve_more\",\n",
        "           False: \"generate\"\n",
        "       }\n",
        "   )\n",
        "   ```\n",
        "\n",
        "2. **Human-in-the-Loop**:\n",
        "   ```python\n",
        "   def needs_clarification(state):\n",
        "       return is_ambiguous(state[\"question\"])\n",
        "   \n",
        "   workflow.add_node(\"clarify\", ask_user_for_clarification)\n",
        "   ```\n",
        "\n",
        "3. **Cycles and Loops**:\n",
        "   ```python\n",
        "   def should_continue(state):\n",
        "       return not state.get(\"satisfied\", False)\n",
        "   \n",
        "   workflow.add_edge(\"generate\", \"retrieve\")  # Loop back\n",
        "   ```\n",
        "\n",
        "### Benefits of LangGraph:\n",
        "\n",
        "- **Flexibility**: Handle complex, non-linear workflows\n",
        "- **State Persistence**: Maintain context across steps\n",
        "- **Debugging**: Easy to trace execution flow\n",
        "- **Scalability**: Can handle complex multi-step processes\n",
        "- **Reusability**: Workflow components can be reused\n",
        "- **Testing**: Easy to test individual nodes\n",
        "\n",
        "### Real-World Example:\n",
        "\n",
        "Our Greek Derby chatbot could be enhanced with:\n",
        "\n",
        "```python\n",
        "# Enhanced workflow\n",
        "def enhanced_rag_workflow():\n",
        "    workflow = StateGraph(GreekDerbyState)\n",
        "    \n",
        "    # Add more nodes\n",
        "    workflow.add_node(\"classify\", classify_question_type)\n",
        "    workflow.add_node(\"retrieve\", retrieve_documents)\n",
        "    workflow.add_node(\"validate\", validate_context)\n",
        "    workflow.add_node(\"generate\", generate_answer)\n",
        "    workflow.add_node(\"fact_check\", fact_check_answer)\n",
        "    \n",
        "    # Complex flow\n",
        "    workflow.add_edge(START, \"classify\")\n",
        "    workflow.add_conditional_edges(\"classify\", route_by_type)\n",
        "    workflow.add_edge(\"retrieve\", \"validate\")\n",
        "    workflow.add_conditional_edges(\"validate\", needs_more_context)\n",
        "    workflow.add_edge(\"generate\", \"fact_check\")\n",
        "    workflow.add_edge(\"fact_check\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q6: How do we manage conversation memory in our RAG chatbot?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Conversation memory is crucial for creating engaging, context-aware chatbots. Our Greek Derby RAG chatbot uses LangChain's memory system to remember previous interactions and maintain conversation context.\n",
        "\n",
        "### Why Memory Matters:\n",
        "\n",
        "**Without Memory:**\n",
        "```\n",
        "User: \"What is the history of the derby?\"\n",
        "Bot: \"The Greek Derby between Olympiakos and Panathinaikos...\"\n",
        "\n",
        "User: \"Who has won more times?\"\n",
        "Bot: \"I need more context. Which teams are you asking about?\"\n",
        "```\n",
        "\n",
        "**With Memory:**\n",
        "```\n",
        "User: \"What is the history of the derby?\"\n",
        "Bot: \"The Greek Derby between Olympiakos and Panathinaikos...\"\n",
        "\n",
        "User: \"Who has won more times?\"\n",
        "Bot: \"Based on our previous discussion about the Greek Derby, Olympiakos has won more times...\"\n",
        "```\n",
        "\n",
        "### Our Memory Implementation:\n",
        "\n",
        "```python\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "def _init_memory(self):\n",
        "    \"\"\"Initialize conversation memory\"\"\"\n",
        "    self.memory = ConversationBufferMemory(\n",
        "        return_messages=True,\n",
        "        memory_key=\"chat_history\",\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "```\n",
        "\n",
        "### Memory Types in LangChain:\n",
        "\n",
        "1. **ConversationBufferMemory** (what we use):\n",
        "   - Stores all conversation history\n",
        "   - Simple but can get large\n",
        "   - Good for short conversations\n",
        "\n",
        "2. **ConversationBufferWindowMemory**:\n",
        "   - Stores only last N messages\n",
        "   - Prevents memory from growing too large\n",
        "   - Good for long conversations\n",
        "\n",
        "3. **ConversationSummaryMemory**:\n",
        "   - Summarizes old conversations\n",
        "   - Keeps recent messages + summary\n",
        "   - Good for very long conversations\n",
        "\n",
        "4. **ConversationTokenBufferMemory**:\n",
        "   - Stores messages up to token limit\n",
        "   - Automatically manages size\n",
        "   - Good for cost control\n",
        "\n",
        "### How Our Memory Works:\n",
        "\n",
        "```python\n",
        "def chat(self, question: str) -> str:\n",
        "    \"\"\"Main chat function with memory\"\"\"\n",
        "    # Get conversation history\n",
        "    chat_history = self.memory.chat_memory.messages\n",
        "    \n",
        "    # Create prompt with history\n",
        "    prompt = self._create_prompt_with_history(question, chat_history)\n",
        "    \n",
        "    # Generate response\n",
        "    response = self.llm.invoke(prompt)\n",
        "    \n",
        "    # Save to memory\n",
        "    self.memory.save_context(\n",
        "        {\"input\": question},\n",
        "        {\"output\": response}\n",
        "    )\n",
        "    \n",
        "    return response\n",
        "```\n",
        "\n",
        "### Memory Storage:\n",
        "\n",
        "Our memory stores:\n",
        "- **User Messages**: What the user asked\n",
        "- **Bot Responses**: What the bot answered\n",
        "- **Timestamps**: When each message was sent\n",
        "- **Context**: Retrieved documents for each question\n",
        "\n",
        "### Memory in Prompts:\n",
        "\n",
        "```python\n",
        "def _create_prompt_with_history(self, question: str, history: List[BaseMessage]) -> str:\n",
        "    \"\"\"Create prompt including conversation history\"\"\"\n",
        "    \n",
        "    # Format history\n",
        "    history_text = \"\"\n",
        "    for message in history[-6:]:  # Last 6 messages\n",
        "        if isinstance(message, HumanMessage):\n",
        "            history_text += f\"User: {message.content}\\n\"\n",
        "        elif isinstance(message, AIMessage):\n",
        "            history_text += f\"Assistant: {message.content}\\n\"\n",
        "    \n",
        "    # Create full prompt\n",
        "    prompt = f\"\"\"\n",
        "    ŒïŒØœÉœÑŒµ Œ≠ŒΩŒ±œÇ ŒµŒæŒµŒπŒ¥ŒπŒ∫ŒµœÖŒºŒ≠ŒΩŒøœÇ Œ≤ŒøŒ∑Œ∏œåœÇ Œ≥ŒπŒ± œÑŒø ŒµŒªŒªŒ∑ŒΩŒπŒ∫œå œÄŒøŒ¥œåœÉœÜŒ±ŒπœÅŒø Œ∫Œ±Œπ œÑŒø ŒΩœÑŒ≠œÅŒºœÄŒπ ŒüŒªœÖŒºœÄŒπŒ±Œ∫œåœÇ-Œ†Œ±ŒΩŒ±Œ∏Œ∑ŒΩŒ±œäŒ∫œåœÇ.\n",
        "    \n",
        "    Œ†œÅŒøŒ∑Œ≥ŒøœçŒºŒµŒΩŒ∑ œÉœÖŒΩŒøŒºŒπŒªŒØŒ±:\n",
        "    {history_text}\n",
        "    \n",
        "    ŒßœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒÆœÉœÑŒµ œÑŒπœÇ œÄŒ±œÅŒ±Œ∫Œ¨œÑœâ œÄŒªŒ∑œÅŒøœÜŒøœÅŒØŒµœÇ Œ≥ŒπŒ± ŒΩŒ± Œ±œÄŒ±ŒΩœÑŒÆœÉŒµœÑŒµ œÉœÑŒ∑ŒΩ ŒµœÅœéœÑŒ∑œÉŒ∑ œÑŒøœÖ œáœÅŒÆœÉœÑŒ∑.\n",
        "    ŒëœÄŒ±ŒΩœÑŒÆœÉœÑŒµ œÉœÑŒ± ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ ŒºŒµ œÜŒπŒªŒπŒ∫œå Œ∫Œ±Œπ ŒµŒΩŒ∑ŒºŒµœÅœâœÑŒπŒ∫œå œÑœÅœåœÄŒø.\n",
        "    \n",
        "    Œ†ŒµœÅŒπŒµœáœåŒºŒµŒΩŒø: {{context}}\n",
        "    ŒïœÅœéœÑŒ∑œÉŒ∑: {question}\n",
        "    ŒëœÄŒ¨ŒΩœÑŒ∑œÉŒ∑:\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "```\n",
        "\n",
        "### Memory Management Functions:\n",
        "\n",
        "```python\n",
        "def get_conversation_history(self) -> List[Dict[str, str]]:\n",
        "    \"\"\"Get formatted conversation history\"\"\"\n",
        "    messages = self.memory.chat_memory.messages\n",
        "    history = []\n",
        "    \n",
        "    for i in range(0, len(messages), 2):\n",
        "        if i + 1 < len(messages):\n",
        "            history.append({\n",
        "                \"user\": messages[i].content,\n",
        "                \"bot\": messages[i + 1].content\n",
        "            })\n",
        "    \n",
        "    return history\n",
        "\n",
        "def clear_memory(self):\n",
        "    \"\"\"Clear conversation memory\"\"\"\n",
        "    self.memory.clear()\n",
        "\n",
        "def get_stats(self) -> str:\n",
        "    \"\"\"Get conversation statistics\"\"\"\n",
        "    messages = self.memory.chat_memory.messages\n",
        "    user_messages = [m for m in messages if isinstance(m, HumanMessage)]\n",
        "    bot_messages = [m for m in messages if isinstance(m, AIMessage)]\n",
        "    \n",
        "    return f\"\"\"\n",
        "    Œ£œÖŒΩŒøŒºŒπŒªŒØŒ±:\n",
        "    ŒïœÅœâœÑŒÆœÉŒµŒπœÇ: {len(user_messages)}\n",
        "    ŒëœÄŒ±ŒΩœÑŒÆœÉŒµŒπœÇ: {len(bot_messages)}\n",
        "    ŒûŒµŒ∫ŒØŒΩŒ∑œÉŒµ: {messages[0].additional_kwargs.get('timestamp', 'Unknown')}\n",
        "    Œ§ŒµŒªŒµœÖœÑŒ±ŒØŒ± Œ¥œÅŒ±œÉœÑŒ∑œÅŒπœåœÑŒ∑œÑŒ±: {messages[-1].additional_kwargs.get('timestamp', 'Unknown')}\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "### Memory Benefits:\n",
        "\n",
        "1. **Context Awareness**: Bot remembers what was discussed\n",
        "2. **Follow-up Questions**: Can answer \"Who is that?\" after discussing a player\n",
        "3. **Personalization**: Can adapt responses based on user's interests\n",
        "4. **Continuity**: Conversations feel natural and flowing\n",
        "5. **User Experience**: More engaging and human-like interactions\n",
        "\n",
        "### Memory Limitations:\n",
        "\n",
        "1. **Size Growth**: Memory can get very large over time\n",
        "2. **Cost**: More memory = more tokens = higher cost\n",
        "3. **Relevance**: Old information might not be relevant\n",
        "4. **Privacy**: Sensitive information stored in memory\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "1. **Limit History**: Only keep recent relevant messages\n",
        "2. **Summarize Old**: Compress old conversations\n",
        "3. **Clear Option**: Allow users to clear memory\n",
        "4. **Context Filtering**: Only include relevant history\n",
        "5. **Token Management**: Monitor and control token usage\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
